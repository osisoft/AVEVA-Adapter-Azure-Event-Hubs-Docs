{
    "content/configuration/configuration.html":  {
                                                     "href":  "content/configuration/configuration.html",
                                                     "title":  "Configuration",
                                                     "keywords":  "Configuration PI Adapter for Azure Event Hubs provides configuration of data source and data selection. The examples in the configuration topics use curl , a commonly available tool on both Windows and Linux. You can configure the adapter with any programming language or tool that supports making REST calls or with the EdgeCmd utility. For more information, see the EdgeCmd utility documentation . To validate successful configurations, you can perform data retrieval ( GET commands) with a browser, if available, on your device. For more information on PI Adapter configuration tools, see Configuration tools . Quick start This Quick Start guides you through setup of each configuration file available for PI Adapter for Azure Event Hubs. As you complete each step, perform each required configuration to establish a data flow from a data source to one or more endpoints. Some configurations are optional. Important: If you want to complete the optional configurations, complete those tasks before the required tasks. Configure one or more Azure Event Hubs system components. See System components . Configure an Azure Event Hubs data source for each Azure Event Hubs device. See Data source . Optional : Configure client settings. See Client settings . Configure an Azure Event Hubs data selection for each Azure Event Hubs data source. See Data selection . Optional : Configure data filters, diagnostics and metadata, buffering, and logging. See the following topics: Data filters Diagnostics and metadata Buffering Logging Configure one or more egress and health endpoints. If there is a proxy between the adapter and your egress endpoints, define it. See the following topics: Egress endpoints Configure a network proxy Health endpoints"
                                                 },
    "content/configuration/pi-adapter-for-azure-event-hubs-client-settings-configuration.html":  {
                                                                                                     "href":  "content/configuration/pi-adapter-for-azure-event-hubs-client-settings-configuration.html",
                                                                                                     "title":  "Client settings",
                                                                                                     "keywords":  "Client settings The client settings configuration is automatically generated when you add a new data source. If you experience problems with timeouts or when Azure Event Hubs limits are exceeded in terms of browse or subscription operation, you can change the client settings configuration. Configure client settings Complete the following steps to configure Azure Event Hubs client settings. Use the PUT method in conjunction with the api/v1/configuration/\u003cComponentId\u003e/ClientSettings api v1 configuration \u003cComponentId\u003e ClientSettings REST endpoint to initialize the configuration. Using a text editor, create an empty text file. Copy and paste an example configuration for Azure Event Hubs client settings into the file. For sample JSON, see Client settings example . Update the example JSON parameters for your environment. For a table of all available parameters, see Client settings parameters . Save the file. For example, as ConfigureClientSettings.json . Open a command line session. Change the directory to the location of ConfigureClientSettings.json . Enter the following cURL command (which uses the PUT method) to initialize the client settings configuration. curl -d \"@ConfigureClientSettings.json\" -H \"Content-Type: application/json\" application json\" -X PUT \"http://localhost:5590/api/v1/configuration/AzureEventHubs1/ClientSettings\" \"http:  localhost:5590 api v1 configuration AzureEventHubs1 ClientSettings\" Notes: If you installed the adapter to listen on a non-default port, update 5590 to the port number you used. If you use a component ID other than AzureEventHubs1 , update the endpoint with your chosen component ID. For a list of other REST operations you can perform, like updating or deleting a client settings configuration, see REST URLs . Client settings schema The full schema definition for the Azure Event Hubs client settings configuration is in the EventHubs_ClientSettings_schema.json file located in one of the following folders: Windows: %ProgramFiles%\\OSIsoft\\Adapters\\EventHubs\\Schemas Linux: /opt/OSIsoft/Adapters/EventHubs/Schemas  opt OSIsoft Adapters EventHubs Schemas Client settings parameters The following parameters are available for configuring Azure Event Hubs client settings: Parameter Required Type Description MaxInternalQueueSize Optional integer Specifies maximum number of value updates that can be held by the adapter in memory Allowed value: \u003e 1000 Default value: 500000 InternalQueuePollingIntervalInMs Optional string (time-span) Specified interval at which data is removed from internal queue Allowed value: Between 1 and 60000 milliseconds Default value: 0:00:00.05 TrackLastEnqueuedEventProperties Optional boolean Indicates if the user should request information on the last enqueued event on the partition associated with a given event and track that information as events are received Allowed value: true or false Default value: false CacheEventCount Optional integer The maximum amount of events read from the Event Hubs service and held in a local memory cache when reading is active and events are emitted to an enumerator for processing Default value: 100 PrefetchCount Optional integer The number of events requested from the Event Hubs service and staged locally regardless of whether a reader is active. PrefetchCount is intended to maximize throughput by buffering service operations. Allowed value: Must be greater than 2 times the CacheEventCount . Default value: 300 BatchSizeForCheckpoint Optional integer The number of events processed to trigger a checkpoint operation Default value: 500 CheckpointingTimeoutSeconds Optional integer The timeout in seconds for the checkpoint operation Default value: 60 DeviceIdSystemPropertyName Optional string The name of the system property for the device id. Default value: iothub-connection-device-id EventHubTransportType Optional enum Identifies the protocol used internally by the client. Although this setting defaults to AmqpTcp , AmqpWebSockets may improve performance in some cases. Allowed values: AmqpTcp and AmqpWebSockets Default value: AmqpTcp EventHubLoadBalancingStrategy Optional enum The load balancing strategy used by the internal Event Hub client used by the adapter. For more information, see Microsoft\u0027s LoadBalancingStrategy Enum . Note: This setting is irrelevant when you use the recommended Event Hub configuration (single partition). Allowed values: Balanced or Greedy Default value: Greedy EventProcessorClientMaximumRetries Optional integer The maximum amount of retries that the EventProcessorClient makes in case of client failures Default value: 5 EventProcessorClientMaximumDelayInMin Optional integer The maximum delay for the timeout operation in minutes Default value: 5 Client settings example { \"MaxInternalQueueSize\" : 500000, \"InternalQueuePollingIntervalInMs\" : \"0:00:00.05\", \"TrackLastEnqueuedEventProperties\" : false, \"CacheEventCount\" : 100, \"PrefetchCount\" : 300, \"BatchSizeForCheckpoint\" : 500, \"CheckpointingTimeoutSeconds\" : 60, \"DeviceIdSystemPropertyName\" : \"iothub-connection-device-id\", \"EventHubTransportType\" : \"AmqpTcp\", \"EventHubLoadBalancingStrategy\": \"Greedy\", \"EventProcessorClientMaximumRetries\" : 5, \"EventProcessorClientMaximumDelayInMin\" : 5 } REST URLs Relative URL HTTP verb Action api/v1/configuration/ api v1 configuration  ComponentId /ClientSettings  ClientSettings GET Retrieves the Azure Event Hubs client settings configuration api/v1/configuration/ api v1 configuration  ComponentId /ClientSettings  ClientSettings PUT Configures or updates the Azure Event Hubs client settings configuration api/v1/configuration/ api v1 configuration  ComponentId /ClientSettings  ClientSettings DELETE Deletes the Azure Event Hubs client settings configuration api/v1/configuration/ api v1 configuration  ComponentId /ClientSettings  ClientSettings PATCH Allows partial updating of configured client settings fields Note: Replace ComponentId with the Id of your Azure Event Hubs component, for example AzureEventHubs1."
                                                                                                 },
    "content/configuration/pi-adapter-for-azure-event-hubs-configuration-examples.html":  {
                                                                                              "href":  "content/configuration/pi-adapter-for-azure-event-hubs-configuration-examples.html",
                                                                                              "title":  "Configuration examples",
                                                                                              "keywords":  "Configuration examples The following tables provide examples for all configurations available for PI Adapter for Azure Event Hubs. Note: The examples in this topic are using the default port number 5590 . If you selected a different port number, replace it with that value. System components configuration The following JSON is an example of a system components configuration. It contains two instances of the Azure Event Hubs adapter. [ { \"ComponentId\": \"EventHubs1\", \"ComponentType\": \"EventHubs\" }, { \"ComponentId\": \"EventHubs2\", \"ComponentType\": \"EventHubs\" }, { \"ComponentId\": \"OmfEgress\", \"ComponentType\": \"OmfEgress\" } ] Adapter configuration { \"AzureEventHubs1\": { \"Logging\": { \"logLevel\": \"Information\", \"logFileSizeLimitBytes\": 34636833, \"logFileCountLimit\": 31 }, \"DataSource\": { \"StreamIdPrefix\": \"EventHubs1.\", \"DefaultStreamIdPattern\": \"{EventHubName}.{ValueField}\", \"EventHubNamespaceConnectionString\": \"\u003cAzure Event Hub Namespace connection string\u003e\", \"BlobStorageConnectionString\": \"\u003cAzure Storage Account connection string\u003e\", \"ConsumerGroupName\": \"$Default\", \"CheckpointBlobContainerName\": \"\u003ccheckpoint container\u003e\", \"TimeZone\": \"America/Los_Angeles\" \"America Los_Angeles\" }, \"DataSelection\": [ { \"Selected\" : true, \"Name\" : null, \"StreamId\" : \"SampleStreamId\", \"DataFilterId\" : null, \"EventHubName\" : \"SampleEventHubName\", \"ValueField\": \"$.Events[0].Value\", \"IndexField\": \"$.TimeStamp\", \"DeviceId\" : \"EventHub7\", \"DataType\" : \"uint64\", \"IndexFormat\" : null } ] }, \"System\": { \"Logging\": { \"logLevel\": \"Information\", \"logFileSizeLimitBytes\": 34636833, \"logFileCountLimit\": 31 }, \"HealthEndpoints\": [ ], \"Diagnostics\": { \"enableDiagnostics\": true }, \"Components\": [ { \"componentId\": \"Egress\", \"componentType\": \"OmfEgress\" }, { \"componentId\": \"AzureEventHubs1\", \"componentType\": \"AzureEventHubs\" } ], \"Buffering\": { \"BufferLocation\": \"C:/ProgramData/OSIsoft/Adapters/AzureEventHubs/Buffers\", \"C: ProgramData OSIsoft Adapters AzureEventHubs Buffers\", \"MaxBufferSizeMB\": -1, \"EnableBuffering\": true } }, \"OmfEgress\": { \"Logging\": { \"logLevel\": \"Information\", \"logFileSizeLimitBytes\": 34636833, \"logFileCountLimit\": 31 }, \"DataEndpoints\": [ { \"id\": \"WebAPI EndPoint\", \"endpoint\": \"https://PIWEBAPIServer/piwebapi/omf\", \"https:  PIWEBAPIServer piwebapi omf\", \"userName\": \"USERNAME\", \"password\": \"PASSWORD\" }, { \"id\": \"OCS Endpoint\", \"endpoint\": \"https://OCSEndpoint/omf\", \"https:  OCSEndpoint omf\", \"clientId\": \"CLIENTID\", \"clientSecret\": \"CLIENTSECRET\" } ] } } Data source configuration The following are representations of minimal and complete data source configurations of Azure Event Hubs adapter. Minimal data source configuration { \"EventHubNamespaceConnectionString\": \"\u003cAzure Event Hub Namespace connection string\u003e\", \"BlobStorageConnectionString\": \"\u003cAzure Storage Account connection string\u003e\", \"ConsumerGroupName\": \"$Default\", \"CheckpointBlobContainerName\": \"\u003ccheckpoint container\u003e\" } Complete data source configuration { \"StreamIdPrefix\": \"EventHubs1.\", \"DefaultStreamIdPattern\": \"{EventHubName}.{ValueField}\", \"EventHubNamespaceConnectionString\": \"\u003cAzure Event Hub Namespace connection string\u003e\", \"BlobStorageConnectionString\": \"\u003cAzure Storage Account connection string\u003e\", \"ConsumerGroupName\": \"$Default\", \"CheckpointBlobContainerName\": \"\u003ccheckpoint container\u003e\", \"TimeZone\": \"America/Los_Angeles\" \"America Los_Angeles\" } Client settings configuration { \"MaxInternalQueueSize\" : 500000, \"InternalQueuePollingIntervalInMs\" : \"0:00:00.05\", \"TrackLastEnqueuedEventProperties\" : false, \"CacheEventCount\" : 100, \"PrefetchCount\" : 300, \"BatchSizeForCheckpoint\" : 500, \"CheckpointingTimeoutSeconds\" : 60, \"DeviceIdSystemPropertyName\" : \"iothub-connection-device-id\", \"EventHubTransportType\" : \"AmqpTcp\", \"EventHubLoadBalancingStrategy\": \"Greedy\", \"EventProcessorClientMaximumRetries\" : 5, \"EventProcessorClientMaximumDelayInMin\" : 5 } Data selection configuration The following are representations of minimal and complete data selection configurations of Azure Event Hubs adapter. Minimal data selection configuration [ { \"EventHubName\" : \"SampleEventHubName\", \"ValueField\" : \"$.TestNode[:1].Value\", \"DataType\" : \"uint64\" } ] Complete data selection configuration [ { \"Selected\" : true, \"Name\" : null, \"StreamId\" : \"SampleStreamId\", \"DataFilterId\" : null, \"EventHubName\" : \"SampleEventHubName\", \"ValueField\": \"$.Events[0].Value\", \"IndexField\": \"$.TimeStamp\", \"DeviceId\" : \"EventHub7\", \"DataType\" : \"uint64\", \"IndexFormat\" : null } ]"
                                                                                          },
    "content/configuration/pi-adapter-for-azure-event-hubs-data-selection-configuration.html":  {
                                                                                                    "href":  "content/configuration/pi-adapter-for-azure-event-hubs-data-selection-configuration.html",
                                                                                                    "title":  "Data selection",
                                                                                                    "keywords":  "Data selection In addition to the data source configuration, you need to provide a data selection configuration to specify the data you want the adapter to collect from the data sources. To configure data selection Complete the following steps to configure an Azure Event Hubs data selection. Use the PUT method in conjunction with the api/v1/configuration/\u003cComponentId\u003e/DataSelection api v1 configuration \u003cComponentId\u003e DataSelection REST endpoint to initialize the configuration. Using a text editor, create an empty text file. Copy and paste an example configuration for an Azure Event Hubs data selection into the file. For sample JSON, see Data selection examples . Update the example JSON parameters for your environment. For a table of all available parameters, see Data selection parameters . Save the file. For example, as ConfigureClientSettings.json . Open a command line session. Change the directory to the location of ConfigureClientSettings.json . Enter the following cURL command (which uses the PUT method) to initialize the client settings configuration. curl -d \"@ConfigureClientSettings.json\" -H \"Content-Type: application/json\" application json\" -X PUT \"http://localhost:5590/api/v1/configuration/AzureEventHubs1/DataSelection\" \"http:  localhost:5590 api v1 configuration AzureEventHubs1 DataSelection\" Notes: If you installed the adapter to listen on a non-default port, update 5590 to the port number you used. If you use a component ID other than AzureEventHubs1 , update the endpoint with your chosen component ID. For a list of other REST operations you can perform, like updating or deleting a client settings configuration, see REST URLs . Data selection schema The full schema definition for the Azure Event Hubs data selection configuration is in the EventHubs_DataSelection_schema.json file located in one of the following folders: Windows: %ProgramFiles%\\OSIsoft\\Adapters\\EventHubs\\Schemas Linux: /opt/OSIsoft/Adapters/EventHubs/Schemas  opt OSIsoft Adapters EventHubs Schemas Data selection parameters Parameter Required Type Description Selected Optional boolean Selects or clears a measurement. To select an item, set to true . To remove an item, leave the field empty or set to false . Allowed value: true or false Default value: false Name Optional string The optional friendly name of the data item collected from the data source Allowed value: any string Default value: null StreamId Optional string The custom identifier used to create the streams. If you do not specify the StreamID or define it as null , the adapter generates a default StreamId based on the measurement configuration. Additionally, within this default StreamId value, the text parser replaces any automatically generated special characters with a replacement character. You can override these replacement characters by manually editing the data selection configuration. For more information on character replacement, see Special characters support . A properly configured custom identifier follows the stream ID rules : Is not case-sensitive Can contain spaces Cannot start with two underscores (\"__\") Can contain a maximum of 100 characters Cannot start or end with a period Cannot contain consecutive periods Cannot consist of only periods. The default ID automatically updates when there are changes to the measurement and follows the format of \u003cId\u003e.\u003cValueField\u003e . Note: At time of data egress, the adapter encodes any invalid characters within the StreamId . For more information, see Egress endpoints . DataFilterId Optional string The ID of the data filter Allowed value: any string Default value: null Note: If the specified DataFilterId does not exist, unfiltered data is sent until that DataFilterId is created. EventHubName Required string The name of the event hub to collect data from Allowed value: Maximum of 256 characters per Azure limits Default value: {EventHubName} ValueField 1 string The JSONPath expression 2 to take value from a property DataFields 1 string A ComplexTypeMapping that maps JSONPath expressions of fields to property names. Supported complex data types are Coordinates (x, y, and z) or Geolocation (latitude and longitude). Complex data type field mapping examples IndexField Optional string The JSONPath expression 2 to take value to use as a timestamp from a property Note: The adapter generates a timestamp when null is specified. DeviceId Optional string The device Id associated with the IoT Hub. DataType Required string The expected data type of the values for the specified field. Supported Azure Event Hub data types include: Boolean , Int64 , Int32 , Int16 , UInt64 , UInt32 , UInt16 , Float64 , Float32 , Float16 , Date-Time , String . For more information, see Principles of operation . IndexFormat Optional string The time format of the timestamp value specified in the IndexField property. Allowed value: Any string that can be used as a DateTime format in the .NET DateTime.TryParseExact() method, for example 01/30/2021 01 30 2021 . For more information, see Date and time processing . 2 Note: If you specify null , the adapter parses the timestamp identified in IndexField as a DateTime supporting ISO 8601 formats. 1 : DataFields and ValueField are mutually exclusive. You must define one or the other, but not both. 2 : JSONPath expressions can be expensive to evaluate. For the best performance, avoid complicated expressions in favor of direct references to data. Data selection examples The following are examples of valid Azure Event Hubs data selection configurations: Minimal data selection configuration [ { \"EventHubName\" : \"SampleEventHubName\", \"ValueField\" : \"$.TestNode[:1].Value\", \"DataType\" : \"uint64\" } ] Complete data selection configuration [ { \"Selected\" : true, \"Name\" : null, \"StreamId\" : \"SampleStreamId\", \"DataFilterId\" : null, \"EventHubName\" : \"SampleEventHubName\", \"ValueField\": \"$.Events[0].Value\", \"IndexField\": \"$.TimeStamp\", \"DeviceId\" : \"EventHub7\", \"DataType\" : \"uint64\", \"IndexFormat\" : null } ] Note: Both ValueField and IndexField require the correct structure of the JSON payload to be specified. The previous examples use the following JSON payload structure: { \"TimeStamp\": \"02/17/2021 \"02 17 2021 12:01:36 AM PST\", \"Events\": [ { \"Value\": \"4578\", \"DataType\": \"int\" } ] } Complex data type field mapping examples When working with the DataFields or DataType data selection parameters , you can provide complex data types field mappings as JSONPath expressions. PI Adapter for Azure Event Hubs supports the following complex data types: Coordinates and Geolocation . Coordinates example {\"X\": \"$[\u0027xValue\u0027]\", \"Y\": \"$[\u0027yValue\u0027]\", \"Z\": \"$[\u0027zValue\u0027]\"} Geolocation example {\"Latitude\": \"$[\u0027latitudeValue\u0027]\", \"Longitude\": \"$[\u0027longitudeValue\u0027]\"} REST URLs Relative URL HTTP verb Action api/v1/configuration/\u003cComponentId\u003e/DataSelection api v1 configuration \u003cComponentId\u003e DataSelection GET Retrieves the data selection configuration, including all data selection items. api/v1/configuration/\u003cComponentId\u003e/DataSelection api v1 configuration \u003cComponentId\u003e DataSelection PUT Configures or updates the data selection configuration. The adapter starts collecting data for each data selection item when the following conditions are met: ??? The data selection configuration PUT request is received. ??? A data source configuration is active. api/v1/configuration/\u003cComponentId\u003e/DataSelection api v1 configuration \u003cComponentId\u003e DataSelection DELETE Deletes the active data selection configuration. The adapter stops collecting data. api/v1/configuration/\u003cComponentId\u003e/DataSelection api v1 configuration \u003cComponentId\u003e DataSelection PATCH Allows partial updates of configured data selection items. Note: The request must be an array containing one or more data selection items. Each item in the array must include its StreamId . api/v1/configuration/\u003cComponentId\u003e/DataSelection/\u003cStreamId\u003e api v1 configuration \u003cComponentId\u003e DataSelection \u003cStreamId\u003e PUT Updates or creates a new data selection item by StreamId . For new items, the adapter starts collecting data after the request is received. api/v1/configuration/\u003cComponentId\u003e/DataSelection/\u003cStreamId\u003e api v1 configuration \u003cComponentId\u003e DataSelection \u003cStreamId\u003e DELETE Deletes a data selection item from the configuration by StreamId . The adapter stops collecting data for the deleted item. Note: Replace \u003cComponentId\u003e with the Id of your Azure Event Hubs component, for example AzureEventHubs1."
                                                                                                },
    "content/configuration/pi-adapter-for-azure-event-hubs-data-source-configuration.html":  {
                                                                                                 "href":  "content/configuration/pi-adapter-for-azure-event-hubs-data-source-configuration.html",
                                                                                                 "title":  "Data source",
                                                                                                 "keywords":  "Data source To use the adapter, you must configure the data source from which it polls data, an event hub, and a storage account. For more information on event hubs and the storage account, see PI Adapter for Azure Event Hubs principles of operation . Configure the data source Complete the following steps to configure an Azure Event Hubs data source. Use the PUT method in conjunction with the api/v1/configuration/\u003cComponentId\u003e/DataSource api v1 configuration \u003cComponentId\u003e DataSource REST endpoint to initialize the configuration. Using a text editor, create an empty text file. Copy and paste an example configuration for an Azure Event Hubs data source into the file. For sample JSON, see Data source examples Update the example JSON parameters for your environment. For a table of all available parameters, see Data source parameters Save the file. For example, as ConfigureDataSource.json . Open a command line session. Change the directory to the location of ConfigureDataSource.json . Enter the following cURL command (which uses the PUT method) to initialize the data source configuration. curl -d \"@ConfigureDataSource.json\" -H \"Content-Type: application/json\" application json\" -X PUT \"http://localhost:5590/api/v1/configuration/AzureEventHubs1/DataSource\" \"http:  localhost:5590 api v1 configuration AzureEventHubs1 DataSource\" Notes: If you installed the adapter to listen on a non-default port, update 5590 to the port number you used. If you use a component ID other than AzureEventHubs1 , update the endpoint with your chosen component ID. For a list of other REST operations you can perform, like updating or deleting a data source configuration, see REST URLs . Configure the data selection. For more information, see Data selection . Data source schema The full schema definition for the Azure Event Hubs data source configuration is in the EventHubs_DataSource_schema.json file located in one of the following folders: Windows: %ProgramFiles%\\OSIsoft\\Adapters\\EventHubs\\Schemas Linux: /opt/OSIsoft/Adapters/EventHubs/Schemas  opt OSIsoft Adapters EventHubs Schemas Data source parameters The following parameters are available for configuring an Azure Event Hubs data source: Parameter Required Type Description StreamIdPrefix Optional string Specifies what prefix is used for Stream IDs. The naming convention is {StreamIdPrefix}{StreamId} . An empty string means no prefix will be added to the Stream IDs and names. A null value defaults to ComponentID followed by a period. Example: AzureEventHubs1.{Topic}.{ValueField} Note: Every time you change the StreamIdPrefix of a configured adapter, for example when you delete and add a data source, you need to restart the adapter for the changes to take place. New streams are created on adapter restart and pre-existing streams are no longer updated. Allowed value: any string Default value: null DefaultStreamIdPattern Optional string Specifies the default stream Id pattern to use. Possible parameters: {Topic} , {ValueField} , {DataType} . Allowed value: any string Default value: {Topic}.{ValueField} EventHubNamespaceConnectionString Required string The connection string for the Event Hub namespace Allowed value: The primary or secondary connection string as copied from the Azure portal. Default value: null ConsumerGroupName Required string The name of the consumer group as defined in the Azure Portal for event hub Allowed value: Maximum of 256 characters per Azure limits. Default value: \"$Default\" BlobStorageConnectionString Required string The connection string for the Azure Blob Storage account which is used to store event hub checkpoints Allowed value: The primary or secondary connection string as copied from the Azure portal. Default value: null CheckpointBlobContainerName Required string The name of the container in the Blob Storage account used to store event hub checkpoints Allowed value: 3-63 characters long; contains only letters, numbers, and dash characters per Azure restrictions. TimeZone Optional string The time zone associated with the Event Hub namespace Allowed value: IANA format, for example \"America/Los_Angeles\". \"America Los_Angeles\". For the complete list, see iana Time Zone Database . Data source examples The following are examples of valid Azure Event Hubs data source configurations: Minimal data source configuration { \"EventHubNamespaceConnectionString\": \"\u003cAzureEventHubNamespaceConnectionString\u003e\", \"BlobStorageConnectionString\": \"\u003cAzureStorageAccountConnectionString\u003e\", \"ConsumerGroupName\": \"$Default\", \"CheckpointBlobContainerName\": \"\u003cCheckpointContainer\u003e\" } Complete data source configuration { \"StreamIdPrefix\": \"EventHubs1.\", \"DefaultStreamIdPattern\": \"{EventHubName}.{ValueField}\", \"EventHubNamespaceConnectionString\": \"\u003cAzureEventHubNamespaceConnectionString\u003e\", \"BlobStorageConnectionString\": \"\u003cAzureStorageAccountConnectionString\u003e\", \"ConsumerGroupName\": \"$Default\", \"CheckpointBlobContainerName\": \"\u003cCheckpointContainer\u003e\", \"TimeZone\": \"America/Los_Angeles\" \"America Los_Angeles\" } REST URLs Relative URL HTTP verb Action api/v1/configuration/\u003cComponentId\u003e/DataSource api v1 configuration \u003cComponentId\u003e DataSource GET Retrieves the data source configuration. api/v1/configuration/\u003cComponentId\u003e/DataSource api v1 configuration \u003cComponentId\u003e DataSource POST Creates the data source configuration. The adapter starts collecting data after the following conditions are met: ??? The data source configuration POST request is received. ??? A data selection configuration is active. api/v1/configuration/\u003cComponentId\u003e/DataSource api v1 configuration \u003cComponentId\u003e DataSource PUT Configures or updates the data source configuration. Overwrites any active data source configuration. If no configuration is active, the adapter starts collecting data after the following conditions are met: ??? The data source configuration PUT request is received. ??? A data selection configuration is active. api/v1/configuration/\u003cComponentId\u003e/DataSource api v1 configuration \u003cComponentId\u003e DataSource DELETE Deletes the data source configuration. After the request is received, the adapter stops collecting data. Note: Replace \u003cComponentId\u003e with the Id of your Azure Event Hubs component, for example, AzureEventHubs1."
                                                                                             },
    "content/configuration/pi-adapter-for-azure-event-hubs-data-source-discovery.html":  {
                                                                                             "href":  "content/configuration/pi-adapter-for-azure-event-hubs-data-source-discovery.html",
                                                                                             "title":  "Azure Event Hubs data source discovery",
                                                                                             "keywords":  "Azure Event Hubs data source discovery When running a discovery against an Azure Event Hub namespace defined within your Data source configuration , you can specify query parameters to narrow the scope of the discovery, targeting specific event hubs within the namespace. When the discovery completes, you can add the discovered items to your data selection configuration. Azure Event Hubs query string Azure Event Hubs query strings must include one or more EventHubName , along with a WaitTime that specifies how long the discovery runs. EventHubNames=\u003cEVENT_HUB_1\u003e,\u003cEVENT_HUB_2\u003e;WaitTime=\u003cDAYS\u003e.\u003cHOURS\u003e:\u003cMINUTES\u003e:\u003cSECONDS\u003e Within the discovery request JSON payload, enter the Azure Event Hubs query using the query parameter, as shown in the query examples below. For more information on using the query parameter during API requests to the api/v1/configuration/\u003ccomponentId\u003e/discoveries api v1 configuration \u003ccomponentId\u003e discoveries endpoint, see Discovery . Query parameters When including a query within your discovery, you can add parameters from the table that follows. Separate the EventHubNames and WaitTime parameters with a semicolon ( ; ). String item Required Description EventHubNames Required Specifies one or more event hub as query targets during discovery. Discovery looks for the specified event hubs within the namespace set in the Data source configuration using the EventHubNamespaceConnectionString parameter. ??? When querying for more than one event hub, seperate each event hub with a comma ( , ). ??? The maximum number of event hubs that you can query is ten , as Microsoft Azure limits each namespace to ten event hubs. WaitTime Optional Specifies the duration of the discovery. If you omit the WaitTime parameter, discovery defaults to one minute: 0.00:01:00 . ??? Minimum value: 30 seconds ( 0.00:00:30 ) ??? Maximum value: 7 days ( 7.00:00:00 ). This maximum is limited by Azure Event Hubs, as it retains messages for seven days. Note: Discoveries with WaitTimes outside of the minimum and maximum values are considered invalid and fail with the following message: Discovery operation failed: Exception type: ArgumentException. Message: Wait Time must be between 30 seconds and 7 days . Query examples The following section contains several examples of Azure Event Hub queries. When creating a discovery request, include the Azure Event Hub query within the request payload as shown in Configure discovery . Example 1: Single event hub This Azure Event Hubs query example discovers a single event hub. { \"query\": \"EventHubNames=event-hub-1;WaitTime=0.00:00:30\" } Example 2: Multiple event hubs This Azure Event Hubs query example discovers multiple event hubs. { \"query\": \"EventHubNames=event-hub-1,event-hub-2;WaitTime=0.00:00:30\" } Example 3: No WaitTime This Azure Event Hubs query example discovers multiple event hubs, but omits the WaitTime parameter. Without a specified WaitTime , the discovery defaults to a WaitTime of one minute. { \"query\": \"EventHubNames=event-hub-1,event-hub-2\" } Query results After you submit a discovery request and the discovery completes, you can view its results and use them to create a Data selection configuration . To view results for your discovery and query, perform a GET request against the api/v1/configuration/componentId/discoveries/\u003cdiscoveryId\u003e/result api v1 configuration componentId discoveries \u003cdiscoveryId\u003e result endpoint. For more information, see REST URLs . The discovery results includes a data selection configuration for each stream it finds for an event hub. For example, the following discovery results include a configuration for two streams found for \"eventHubName\":\"event-hub-1\" : \"streamId\": \"event-hub-1.$.PumpTemperature\" and \"streamId\": \"event-hub-1.$.TimeStamp\" . [ {????????? \"eventHubName\": \"event-hub-1\", \"deviceId\": null, \"valueField\": \"$.PumpTemperature\", \"IndexField\": null, \"IndexFormat\": null, \"dataType\": \"Single\", \"selected\": false, \"name\": null, \"streamId\": \"event-hub-1.$.PumpTemperature\", \"dataFilterId\": null }?????????, {????????? \"eventHubName\": \"event-hub-1\", \"deviceId\": null, \"valueField\": \"$.TimeStamp\", \"IndexField\": null, \"IndexFormat\": null, \"dataType\": \"DateTime\", \"selected\": false, \"name\": null, \"streamId\": \"event-hub-1.$.TimeStamp\", \"dataFilterId\": null }????????? ] StreamId special character replacement In query results for streamId , the text parser replaces any automatically generated special characters with a replacement character to make identifiers more meaningful. You can manually override these replacement characters by manually editing the data selection configuration. For more information on character replacement, see Special characters support . For more information on overriding replacement characters within a streamId , see Data selection . Combining query results into a valid data selection configuration After viewing the results of a discovery, you can manually combine them into a single valid data selection configuration. In the example below, the two data selection configurations from the example above are combined into single configuration. Notice that IndexField is updated to $.TimeStamp . After creating a valid data selection configuration, you can activate it by performing a PUT request against the api/v1/configuration/\u003cComponentId\u003e/DataSelection api v1 configuration \u003cComponentId\u003e DataSelection endpoint. For more information, see Data selection . [ { \"eventHubName\": \"event-hub-1\", \"deviceId\": null, \"valueField\": \"$.PumpTemperature\", \"IndexField\": \"$.TimeStamp\", \"IndexFormat\": null, \"dataType\": \"Single\", \"selected\": true, \"name\": null, \"streamId\": \"event-hub-1.$.PumpTemperature\", \"dataFilterId\": null } ] Additional considerations for query and autoselect When you use the query parameter within a discovery request, OSIsoft recommends setting the autoSelect parameter to false for most use cases. OSIsoft makes this recommendation because the data selection configuration that discovery returns includes only values and not indexes. Therefore, the configuration is not suitable for a production environment without manually editing its settings \u003c!--, most notably the `IndexField` setting--\u003e . \"autoSelect\": true should only be used for data sources with simple data structures, as it automatically applies the discovery results as the active Data selection configuration ."
                                                                                         },
    "content/configuration/PIAdapterForAzureEventHubsSecurity.html":  {
                                                                          "href":  "content/configuration/PIAdapterForAzureEventHubsSecurity.html",
                                                                          "title":  "Security",
                                                                          "keywords":  "Security When determining Azure Event Hubs security practices with regards to REST APIs, you should consider the following practice. To keep the adapter secure, only administrators should have access to machines where the adapter is installed. REST APIs are bound to localhost, meaning that only requests coming from within the machine will be accepted."
                                                                      },
    "content/configuration/pi-adapter-for-azure-event-hubs-system-components.html":  {
                                                                                         "href":  "content/configuration/pi-adapter-for-azure-event-hubs-system-components.html",
                                                                                         "title":  "System components",
                                                                                         "keywords":  "System components PI Adapter for Azure Event Hubs uses JSON configuration files in a protected directory on Windows and Linux to store configuration that is read on startup. While the files are accessible to view, OSIsoft recommends that you use REST or the EdgeCmd utility for any changes you make to the files. As part of making adapters as secure as possible, any passwords or secrets that you configure are stored in encrypted form where cryptographic key material is stored separately in a secure location. If you edit the files directly, the adapter may not work as expected. Note: You can edit any single component or facet of the system individually using REST, but you can also configure the system as a whole with a single REST call. Configure system components Complete the following steps to configure system components. Use the PUT method in conjunction with the http://localhost:5590/api/v1/configuration/system/components http:  localhost:5590 api v1 configuration system components REST endpoint to initialize the configuration. Using a text editor, create an empty text file. Copy and paste an example configuration for system components into the file. For sample JSON, see Examples . Update the example JSON parameters for your environment. For a table of all available parameters, see System components parameters . Save the file. For example, as ConfigureComponents.json . Open a command line session. Change directory to the location of ConfigureComponents.json . Enter the following cURL command (which uses the PUT method) to initialize the system components configuration. curl -d \"@ConfigureComponents.json\" -H \"Content-Type: application/json\" application json\" -X PUT \"http://localhost:5590/api/v1/configuration/system/components\" \"http:  localhost:5590 api v1 configuration system components\" Notes: If you installed the adapter to listen on a non-default port, update 5590 to the port number in use. For a list of other REST operations you can perform, like updating or deleting a system components configuration, see REST URLs . System components schema The full schema definition for the system components configuration is in the System_Components_schema.json file located in one of the following folders: Windows: %ProgramFiles%\\OSIsoft\\Adapters\\AdapterName\\Schemas Linux: /opt/OSIsoft/Adapters/AdapterName/Schemas  opt OSIsoft Adapters AdapterName Schemas System components parameters You can configure the following parameters for system components: Parameters Required Type Description ComponentId Required string The ID of the component 1 . It can be any alphanumeric string. A properly configured ComponentID follows these rules: Cannot contain leading or trailing space Cannot use the following characters: \u003e \u003c /   : ? # [ ] @ ! $ \u0026 * \" ( ) \\\\ + , ; = ` ComponentType Required string The type of the component. There are two types of components: OmfEgress and the adapter. 1 1 Note: The OmfEgress component is required to run the adapter. Both its ComponentId and ComponentType are reserved and should not be modified. Examples Default system components configuration The default System_Components.json file for the System component contains the following information. [ { \"ComponentId\": \"OmfEgress\", \"ComponentType\": \"OmfEgress\" } ] System components configuration with two adapter instances [ { \"ComponentId\": \"EventHubs1\", \"ComponentType\": \"EventHubs\" }, { \"ComponentId\": \"EventHubs2\", \"ComponentType\": \"EventHubs\" }, { \"ComponentId\": \"OmfEgress\", \"ComponentType\": \"OmfEgress\" } ] REST URLs Relative URL HTTP verb Action api/v1/configuration/system/components api v1 configuration system components GET Retrieves the system components configuration api/v1/configuration/system/components api v1 configuration system components POST Adds a new component to the system configuration api/v1/configuration/system/components api v1 configuration system components PUT Updates the system components configuration api/v1/configuration/system/components/ api v1 configuration system components  ComponentId DELETE Deletes a specific component from the system components configuration api/v1/configuration/system/components/ api v1 configuration system components  ComponentId PUT Creates a new component with the specified ComponentId in the system configuration"
                                                                                     },
    "content/configuration/pi-adapter-for-azure-event-hubs-text-parser.html":  {
                                                                                   "href":  "content/configuration/pi-adapter-for-azure-event-hubs-text-parser.html",
                                                                                   "title":  "Text parser",
                                                                                   "keywords":  "Text parser The adapter you are using includes the text parser component which ensures consistent parsing of text from different files. For more information on which file types are supported for your adapter, see the topics in this chapter. Designed to be a document parser, the text parser parses a semantically complete document in its entirety. The text parser produces OMF compatible output, which in turn is compatible with the OCS backing SDS (Sequential Data Store) that stores data in streams consisting of multiple values and indexes. Data types supported by the text parser The following data types are supported by the text parser: DateTime DateTimeOffset TimeSpan sbyte byte short ushort int uint long ulong float double decimal bool char string Note: Not all data types supported by the text parser are also supported by OMF. Special characters support As part of the default StreamId logic, the text parser replaces special characters as follows: Special character Replacement character * empty string \u0027 empty string ` empty string \" empty string ? empty string ; - \\| - \\ - { ( } ) [ ( ] ) Time zone support A time zone or offset specified by a time is always used to convert to UTC time. Time zones are only used if there is no offset or time zone specifier in a text date and time string. For time zones that support time changes between daylight and standard times, a text file may temporarily contain invalid or ambiguous datetimes during the time change, which are possible only for a two-hour period each year. When these time changes occur, the text parser logs them, but the datetime is parsed and passed to the callback. Ambiguous times are reported as standard times, which is the Microsoft recommendation. Date and time processing The text parser can use time zones, cultures, and custom formats to read dates and times from ingress data. You can specify date and time formats when you configure data selection. Set the date and time using the IndexFormat property. If you leave the IndexFormat property unset, the data selection configuration defaults to the ISO 8601 date format. If you are using a culture other than default en-US , use the name of day or month specific to the culture. For example, use \"Juni\" instead of \"June\" for the de-DE culture. The following date and time syntaxes have been tested and are supported. \"MM/dd/yyyy \"MM dd yyyy H:mm:ss zzz\" \"06/15/2018 \"06 15 2018 15:15:30 -05:00\" \"MM/dd/yyyy \"MM dd yyyy H:mm:ss.fff zzz\" \"06/15/2018 \"06 15 2018 15:15:30.123 -05:00\" \"dd/MM/yyyy \"dd MM yyyy H:mm:ss.fff K\" \"15/06/2018 \"15 06 2018 15:15:30.123 Z\" \"MMMM/dd/yyyy \"MMMM dd yyyy H:mm:ss.fff K\" \"June/15/2018 \"June 15 2018 15:15:30.123 Z\" (InvariantCulture/English) (InvariantCulture English) \"MMMM/dd/yyyy \"MMMM dd yyyy H:mm:ss.fff K\" \"Juni/15/2018 \"Juni 15 2018 15:15:30.123 Z\" (German) \"MMM/dd/yyyy \"MMM dd yyyy H:mm:ss.fff K\" \"Jun/15/2018 \"Jun 15 2018 15:15:30.123 Z\" \"MMM-dd-yyyy H:mm:ss.fff K\" \"Jun-15-2018 15:15:30.123 Z\" \"MMM-dd-yyyy H:mm:ss.fff K\" \"Jun-15-2018 15:15:30.123 Z\" \"MMM-dd-yyyy H:mm:ss.fff K\" \"Jun-15-2018 15:15:30.123 Z\" \"yyyy-MM-dd H:mm:ss.fff K\" \"2018-06-15 15:15:30.123 Z\" \"yyyy-M-d H:mm:ss.fff K\" \"2018-6-5 15:15:30.123 Z\" \"yyyy-M-d H:mm:ss.fff zzz\" \"2018-6-5 15:15:30.123 +05:00\" \"ddd dd MMM yyyy h:mm tt zzz\" \"Sun 15 Jun 2008 8:30 AM -06:00\" \"dddd dd MMM yyyy h:mm tt zzz\" \"Sunday 15 Jun 2008 8:30 AM -06:00\" \"dddd dd MMM yyyy h:mm tt zzz\" \"Sunday 15 Jun 2008 8:30 AM -06:00\" \"dddd dd MMMM yyyy h:mm tt zzz\" \"Sunday 15 June 2008 8:30 AM -06:00\" Adapter date and time processing uses Microsoft datetime parsing . For more documentation on standard datetime formats, which fit most use cases, see Standard date and time format strings . For documentation on custom datetime formation, see Custom date and time format strings ."
                                                                               },
    "content/health-and-diagnostics/health/adapter-health.html":  {
                                                                      "href":  "content/health-and-diagnostics/health/adapter-health.html",
                                                                      "title":  "Health",
                                                                      "keywords":  "Health PI Adapters produce different kinds of health data that can be egressed to different health endpoints. Available health data The adapter sends dynamic data to configured health endpoints every minute. The following health data is available: PI Adapter for Azure Event Hubs device status Next Health Message Expected AF structure With a health endpoint configured to a PI server, you can use PI System Explorer to view the health of a given adapter. The element hierarchy is shown in the following image."
                                                                  },
    "content/health-and-diagnostics/health/next-health-message-expected.html":  {
                                                                                    "href":  "content/health-and-diagnostics/health/next-health-message-expected.html",
                                                                                    "title":  "Next health message expected",
                                                                                    "keywords":  "Next health message expected This property is similar to a heartbeat. The individual adapter data component sends a new value for NextHealthMessageExpected on a periodic basis while it is functioning properly. This value is a timestamp that indicates when the next value should be received. When monitoring, if the next value is not received by the indicated time, this likely means that there is an issue. It could be an issue with the adapter, adapter component, network connection between the health endpoint and the adapter, and so on. Property Type Description Time string Timestamp of the event NextHealthMessageExpected string Timestamp when next value is expected"
                                                                                },
    "content/health-and-diagnostics/health/pi-adapter-for-azure-event-hubs-device-status.html":  {
                                                                                                     "href":  "content/health-and-diagnostics/health/pi-adapter-for-azure-event-hubs-device-status.html",
                                                                                                     "title":  "Device status",
                                                                                                     "keywords":  "Device status The PI Adapter for Azure Event Hubs device status indicates the health of the Azure Event Hubs component and if it is currently communicating properly with the data source. This time-series data is stored within a PI point or OCS stream depending on the endpoint type. During healthy steady-state operation, a value of Good is expected. Property Type Description Time string Timestamp of the event DeviceStatus string The value of the DeviceStatus The adapter supports the following statuses: Status Meaning Starting The component is starting up and is not yet connected to the data source. ConnectedNoData Connectivity to Event Hubs is confirmed but no events have been received yet. Good Connectivity to Event Hubs is confirmed and event was received DeviceInError The adapter failed to connect to Azure Event Hubs or the connection was interrupted. Shutdown Connectivity to Event Hubs is being closed. NotConfigured The adapter configuration does not have any data selection items defined with Event Hub names. For more information on health data supported by the adapter, see Adapter health ."
                                                                                                 },
    "content/index.html":  {
                               "href":  "content/index.html",
                               "title":  "Overview",
                               "keywords":  "Overview PI Adapter for Azure Event Hubs is a data-collection component that transfers time-series data from Event Hubs to OMF endpoints in OSIsoft Cloud Services or PI Servers. Event Hubs is a data streaming platform and event ingestion service provided by Microsoft as part of the Azure Cloud platform. The adapter can connect to any Event Hub hosted in Azure that uses the Advanced Message Queueing Protocol (AMQP). Adapter installation You can install the adapter with a download kit that you can obtain from the OSIsoft Customer Portal. You can install the adapter on devices running either Windows or Linux operating systems. Adapter configuration Using REST API, you can configure all functions of the adapter. The configurations are stored in JSON files. For data ingress, you must define an adapter component in the system components configuration for each device to which the adapter will connect. You configure each adapter component with the connection information for the device and the data to collect. For data egress, you must specify destinations for the data, including security for the outgoing connection. Additional configurations are available to egress health and diagnostics data, add buffering configuration to protect against data loss, and record logging information for troubleshooting purposes. Once you have configured the adapter and it is sending data, you can use administration functions to manage the adapter or individual ingress components of the adapter. Health and diagnostics functions monitor the status of connected devices, adapter system functions, the number of active data streams, the rate of data ingress, the rate of errors, and the rate of data egress. EdgeCmd utility OSIsoft also provides the EdgeCmd utility, a proprietary command line tool to configure and administer an adapter on both Linux and Windows operating systems. EdgeCmd utility is installed separately from the adapter."
                           },
    "content/main/shared-content/_includes/inline/component-id.html":  {
                                                                           "href":  "content/main/shared-content/_includes/inline/component-id.html",
                                                                           "title":  "",
                                                                           "keywords":  "EventHubs1"
                                                                       },
    "content/main/shared-content/_includes/inline/component-type.html":  {
                                                                             "href":  "content/main/shared-content/_includes/inline/component-type.html",
                                                                             "title":  "",
                                                                             "keywords":  "EventHubs"
                                                                         },
    "content/main/shared-content/_includes/inline/docker-image.html":  {
                                                                           "href":  "content/main/shared-content/_includes/inline/docker-image.html",
                                                                           "title":  "",
                                                                           "keywords":  "eventhubsadapter"
                                                                       },
    "content/main/shared-content/_includes/inline/framework-version.html":  {
                                                                                "href":  "content/main/shared-content/_includes/inline/framework-version.html",
                                                                                "title":  "",
                                                                                "keywords":  "1.4"
                                                                            },
    "content/main/shared-content/_includes/inline/installer-name.html":  {
                                                                             "href":  "content/main/shared-content/_includes/inline/installer-name.html",
                                                                             "title":  "",
                                                                             "keywords":  "PI-Adapter-for-Azure-Event-Hubs-1.0.1.239"
                                                                         },
    "content/main/shared-content/_includes/inline/product-name.html":  {
                                                                           "href":  "content/main/shared-content/_includes/inline/product-name.html",
                                                                           "title":  "",
                                                                           "keywords":  "PI Adapter for Azure Event Hubs"
                                                                       },
    "content/main/shared-content/_includes/inline/product-protocol.html":  {
                                                                               "href":  "content/main/shared-content/_includes/inline/product-protocol.html",
                                                                               "title":  "",
                                                                               "keywords":  ""
                                                                           },
    "content/main/shared-content/_includes/inline/product-version.html":  {
                                                                              "href":  "content/main/shared-content/_includes/inline/product-version.html",
                                                                              "title":  "",
                                                                              "keywords":  "1.0.1.239"
                                                                          },
    "content/main/shared-content/_includes/inline/startup-script.html":  {
                                                                             "href":  "content/main/shared-content/_includes/inline/startup-script.html",
                                                                             "title":  "",
                                                                             "keywords":  "eventhubsdockerstart.sh"
                                                                         },
    "content/main/shared-content/_includes/inline/symantic-version.html":  {
                                                                               "href":  "content/main/shared-content/_includes/inline/symantic-version.html",
                                                                               "title":  "",
                                                                               "keywords":  "1.0.1.239"
                                                                           },
    "content/main/shared-content/administration/administration.html":  {
                                                                           "href":  "content/main/shared-content/administration/administration.html",
                                                                           "title":  "Administration",
                                                                           "keywords":  "Administration With the PI adapter administration level functions, you can start and stop an adapter service and the individual adapter ingress components. You can also retrieve product version information and delete an adapter. The examples in the administration topics use curl , a commonly available tool on both Windows and Linux. You can use the same operations with any programming language or tool that supports making REST calls. You can also configure PI adapters with the EdgeCmd utility. For more information, see the EdgeCmd utility documentation . To validate successful configurations, you can accomplish data retrieval steps ( GET commands) using a browser, if available on your device. For more information on PI adapter configuration tools, see Configuration tools ."
                                                                       },
    "content/main/shared-content/administration/delete-an-adapter-component.html":  {
                                                                                        "href":  "content/main/shared-content/administration/delete-an-adapter-component.html",
                                                                                        "title":  "Delete an adapter component",
                                                                                        "keywords":  "Delete an adapter component When you remove an adapter component, the configuration and log files are saved into a sub-directory in case they are needed later. Any associated types, streams, and data remain on the respective endpoints. Complete the following steps to delete an adapter component: Start any of the Configuration tools capable of making HTTP requests. Run a DELETE command to the following endpoint: http://localhost:5590/api/v1/configuration/system/components/\u003cComponentId\u003e http:  localhost:5590 api v1 configuration system components \u003cComponentId\u003e Note: You must make an empty DELETE command against the Id of the component you want to delete. 5590 is the default port number. If you selected a different port number, replace it with that value. Example using curl : Delete an adapter component curl -X DELETE \"http://localhost:5590/api/v1/configuration/system/components/\u003cComponentId\u003e\" \"http:  localhost:5590 api v1 configuration system components \u003cComponentId\u003e\" File relocation All configuration and log files are renamed and moved. The files are renamed according to the timestamp of removal, for example, FileName.json_removed_yyyy-MM-dd--hh-mm-ss . Configuration files are moved to the following location: Windows: %programdata%\\OSIsoft\\Adapters\\AdapterName\\Configuration\\Removed Linux: /usr/share/OSIsoft/Adapters/AdapterName/Configuration/Removed  usr share OSIsoft Adapters AdapterName Configuration Removed Log files are moved to the following location: Windows: %programdata%\\OSIsoft\\Adapters\\AdapterName\\Logs\\Removed Linux: /usr/share/OSIsoft/Adapters/AdapterName/Logs/Removed  usr share OSIsoft Adapters AdapterName Logs Removed In the following example, one adapter service is installed on a particular Windows node with the name \u003cAdapter\u003eService1 . An adapter component with the name \u003cAdapter\u003eDeviceX was added and configured to this adapter and later removed. Linux follows a similar behavior. This is the resulting relocation and renaming scheme after deletion: REST URLs Relative URL HTTP verb Action api/v1/configuration/system/components/ api v1 configuration system components  ComponentId DELETE Deletes specified component Note: Replace ComponentId with the Id of the component that you want to delete."
                                                                                    },
    "content/main/shared-content/administration/retrieve-product-version-information.html":  {
                                                                                                 "href":  "content/main/shared-content/administration/retrieve-product-version-information.html",
                                                                                                 "title":  "Retrieve product version information",
                                                                                                 "keywords":  "Retrieve product version information The product version information includes the adapter framework version, application version, the version of the underlying .NET Core framework, and the operating system that the adapter is running on. Complete the following steps to retrieve the product version information of a PI adapter: Use any of the Configuration tools capable of making HTTP requests. Run a GET command to the following endpoint: http://localhost:5590/api/v1/Diagnostics/ProductInformation http:  localhost:5590 api v1 Diagnostics ProductInformation Note: 5590 is the default port number. If you selected a different port number, replace it with that value. Example using curl : Get product information for adapter hosted on port 5590 curl -X GET \"http://localhost:5590/api/v1/Diagnostics/ProductInformation\" \"http:  localhost:5590 api v1 Diagnostics ProductInformation\" Example result: { \"Application Name\": \"PI Adapter for \u003cAdapterName\u003e\", \"Adapter Framework Version\": \"1.3.0.351\", \"Application Version\":\"1.2.0.37\", \".Net Core Version\":\".NET Core 3.1.5\", \"Operating System\":\"Linux 4.15.0-106-generic #107-Ubuntu SMP Thu Jun 4 11:27:52 UTC 2020\" }"
                                                                                             },
    "content/main/shared-content/administration/start-and-stop-an-adapter.html":  {
                                                                                      "href":  "content/main/shared-content/administration/start-and-stop-an-adapter.html",
                                                                                      "title":  "Start and stop an adapter",
                                                                                      "keywords":  "Start and stop an adapter Complete the procedure appropriate for your operating system to start or stop an adapter service: Windows Open Windows services. Select PI Adapter for \u003cAdapterName\u003e . Depending on whether your adapter is running or not, click either Start or Stop . Linux Open command line. Depending on whether your adapter is running or not, type one of the following commands: Example: Start PI Adapter for \u003cAdapterName\u003e sudo systemctl start pi.adapter.\u003cadapterName\u003e Example: Stop PI Adapter for \u003cAdapterName\u003e sudo systemctl stop pi.adapter.\u003cadapterName\u003e Press Enter."
                                                                                  },
    "content/main/shared-content/administration/start-and-stop-ingress-component.html":  {
                                                                                             "href":  "content/main/shared-content/administration/start-and-stop-ingress-component.html",
                                                                                             "title":  "Start and stop ingress component",
                                                                                             "keywords":  "Start and stop ingress component To control data ingress, you can start and stop the ingress components of an adapter whenever necessary. By default, all currently configured ingress components are started. Start an ingress component Complete the following steps to start an individual ingress component: Use any of the Configuration tools capable of making HTTP requests. Run a POST command to the following endpoint, replacing \u003cComponentId\u003e with the ingress component that you want to start: http://localhost:5590/api/v1/administration/\u003cComponentId\u003e/Start http:  localhost:5590 api v1 administration \u003cComponentId\u003e Start Note: 5590 is the default port number. If you selected a different port number, replace it with that value. Example using curl : Start the adapter ingress component curl -d \"\" -X POST \"http://localhost:5590/api/v1/Administration/\u003cComponentId\u003e/Start\" \"http:  localhost:5590 api v1 Administration \u003cComponentId\u003e Start\" Stop an ingress component Complete the following steps to stop an individual ingress component: Start any configuration tool capable of making HTTP requests. Run a POST command to the following endpoint, replacing \u003cComponentId\u003e with the ingress component that you want to stop: http://localhost:5590/api/v1/administration/\u003cComponentId\u003e/Stop http:  localhost:5590 api v1 administration \u003cComponentId\u003e Stop Note: 5590 is the default port number. If you selected a different port number, replace it with that value. Example using curl : Stop the adapter ingress component curl -d \"\" -X POST \"http://localhost:5590/api/v1/Administration/\u003cComponentId\u003e/Stop\" \"http:  localhost:5590 api v1 Administration \u003cComponentId\u003e Stop\""
                                                                                         },
    "content/main/shared-content/configuration/buffering.html":  {
                                                                     "href":  "content/main/shared-content/configuration/buffering.html",
                                                                     "title":  "Buffering",
                                                                     "keywords":  "Buffering You can configure PI adapters to buffer data egressed from the adapter to endpoints. Buffering is configured through the buffering configuration parameters in the system configuration. Note: OSIsoft recommends that you do not modify the default buffering location unless it is necessary. Changes to the buffering configuration parameters only take effect during adapter service startup. Configure buffering Complete the following steps to configure buffering. Use the PUT method in conjunction with the http://localhost:5590/api/v1/configuration/system/buffering http:  localhost:5590 api v1 configuration system buffering REST endpoint to initialize the configuration. Using a text editor, create an empty text file. Copy and paste an example configuration for buffering into the file. For sample JSON, see Examples - Retrieve the buffering configuration . Update the example JSON parameters for your environment. For a table of all available parameters, see Buffering parameters . Save the file. For example, as ConfigureBuffering.json . Open a command line session. Change directory to the location of ConfigureBuffering.json . Enter the following cURL command (which uses the PUT method) to initialize the buffering configuration. curl -d \"@ConfigureBuffering.json\" -H \"Content-Type: application/json\" application json\" -X PUT \"http://localhost:5590/api/v1/configuration/system/buffering\" \"http:  localhost:5590 api v1 configuration system buffering\" Notes: If you installed the adapter to listen on a non-default port, update 5590 to the port number in use. For a list of other REST operations you can perform, like updating or replacing a buffering configuration, see REST URLs . Buffering schema The full schema definition for the system buffering is in the System_Buffering_schema.json file located in one of the following folders: Windows: %ProgramFiles%\\OSIsoft\\Adapters\\\u003cAdapterName\u003e\\Schemas Linux: /opt/OSIsoft/Adapters/\u003cAdapterName\u003e/Schemas  opt OSIsoft Adapters \u003cAdapterName\u003e Schemas Buffering parameters The following parameters are available for configuring buffering: Parameter Required Type Description EnablePersistentBuffering Optional boolean Enables or disables on-disk buffering Allowed value: true or false Default value: true Note: If you disable persistent buffering, in-memory buffering is used. On-disk and in-memory buffering are limited by value in the MaxBufferSizeMB property. MaxBufferSizeMB Optional integer Defines the maximum size of the buffer that is persisted on disk 1 or used in memory 2 . The unit is specified in MB (1 Megabyte = 1048576 bytes). Consider the capacity and the type of storage medium to determine a suitable value for this parameter. Minimum value: 1 Maximum value: 2147483647 Default value: 1024 Note: The MaxBufferSizeMB property is applied to each configured endpoint. For example, if you set the MaxBufferSizeMB to 1024 and you configured the adapter to send data to two endpoints (for example, PI Server and OCS), the total maximum resources used for buffering will be 2048 . The health endpoint is an exception fixed at 20 MB. BufferLocation Required string Defines the location of the buffer files. Absolute paths are required. Consider the access-control list (ACL) when you set this parameter. BufferLocation is used to buffer files when EnablePersistentBuffering is true . Allowed value: Valid path to a folder location in the file system Default value: Windows: %ProgramData%\\OSIsoft\\Adapters\\{AdapterInstance}\\Buffers Linux: /usr/share/OSIsoft/Adapters/{AdapterInstance}/Buffers  usr share OSIsoft Adapters {AdapterInstance} Buffers 1 Buffering to disk - disk is only used if required; Data is only written to the disk buffer if queued in the memory buffer for more than 5 seconds. The MaxBufferSizeMB is applied per configured endpoint except the health endpoint. An adapter creates 20 MB buffer files that are stored in BufferLocation . When MaxBufferSizeMB is reached, the oldest buffer file is deleted and a new buffer file is created. The health endpoint is fixed at 20 MB. When the health endpoint buffer file becomes full, a new buffer file is created and the previous buffer file is deleted. Note: The following rules apply in case of an error when creating a new buffer file: Attempt to delete oldest buffer file and retry. If unable to buffer, errors are logged to indicate data loss. If a buffer file is corrupted, an attempt is made to recover individual records and any failure to recover records is logged. 2 Buffering only to memory : The MaxBufferSizeMB is applied per configured endpoint except the health endpoint. When MaxBufferSizeMB is reached, the oldest messages in the memory buffer are removed. Depending on the size of a new message, several old messages may be removed. The health endpoint is fixed at 20 MB. When the health endpoint buffer file becomes full, the oldest messages in the memory buffer are removed and new messages are added. Examples The following examples are buffering configurations made through the curl REST client. Retrieve the buffering configuration curl -X GET \"http://localhost:5590/api/v1/configuration/system/buffering\" \"http:  localhost:5590 api v1 configuration system buffering\" Sample output: { \"bufferLocation\": \"C:/ProgramData/OSIsoft/Adapters/\u003cAdapterName\u003e/Buffers\", \"C: ProgramData OSIsoft Adapters \u003cAdapterName\u003e Buffers\", \"maxBufferSizeMB\": 1024, \"enablePersistentBuffering\": true } 200 OK response indicates success. Update MaxBufferSizeMb parameter curl -d \"{ \\\"MaxBufferSizeMB\\\": 100 }\" -H \"Content-Type: application/json\" application json\" -X PATCH \"http://localhost:5590/api/v1/configuration/system/buffering\" \"http:  localhost:5590 api v1 configuration system buffering\" 204 No Content response indicates success. REST URLs Relative URL HTTP verb Action api/v1/configuration/system/buffering api v1 configuration system buffering GET Gets the buffering configuration api/v1/configuration/system/buffering api v1 configuration system buffering PUT Replaces the existing buffering configuration api/v1/configuration/system/buffering api v1 configuration system buffering PATCH Update parameter, partial configuration"
                                                                 },
    "content/main/shared-content/configuration/configuration-tools.html":  {
                                                                               "href":  "content/main/shared-content/configuration/configuration-tools.html",
                                                                               "title":  "Configuration tools",
                                                                               "keywords":  "Configuration tools You can configure PI adapters with the EdgeCmd utility, OSIsoft\u0027s proprietary tool for configuring adapters, or a commonly-used REST tool. EdgeCmd utility The EdgeCmd utility enables adapter configuration on both Linux and Windows operating systems. For more information on using the EdgeCmd utility, see the EdgeCmd utility documentation . REST tools The following tools are available to make REST calls: curl curl is a command line tool used to make HTTP calls and is supported on both Windows and Linux operating systems. You can script curl with Bash or PowerShell on Linux or Windows and you can use it to perform adapter administrative and programming tasks. curl commands are used in configuration and management examples throughout this document. For more information, see curl (https://curl.haxx.se/) (https:  curl.haxx.se ) . Postman Postman is a REST tool for systems with GUI components. PI adapters are supported on platforms without GUIs. Postman is particularly useful for learning more about PI Adapter REST APIs. For more information, see Postman (https://www.postman.com/) (https:  www.postman.com ) ."
                                                                           },
    "content/main/shared-content/configuration/data-filters.html":  {
                                                                        "href":  "content/main/shared-content/configuration/data-filters.html",
                                                                        "title":  "Data filters",
                                                                        "keywords":  "Data filters PI adapters can be configured to perform data filtering to save network bandwidth. Every data item in the data selection configuration can be assigned the Id of a data filter. The adapter will then filter data for those data items based on the data filter configuration. Note: If data filters are enabled and data quality changes, both the old and current data quality values are passed on. Configure data filters Complete the following steps to configure data filters. Use the PUT method in conjunction with the http://localhost:5590/api/v1/configuration/\u003cComponentId\u003e/DataFilters http:  localhost:5590 api v1 configuration \u003cComponentId\u003e DataFilters REST endpoint to initialize the configuration. Using a text editor, create an empty text file. Copy and paste an example configuration for data filters into the file. For sample JSON, see Data filters example . Update the example JSON parameters for your environment. For a table of all available parameters, see Data filters parameters . Save the file. For example, as ConfigureDataFilters.json . Open a command line session. Change directory to the location of ConfigureDataFilters.json . Enter the following cURL command (which uses the PUT method) to initialize the data filters configuration. curl -d \"@ConfigureDataFilters.json\" -H \"Content-Type: application/json\" application json\" -X PUT \"http://localhost:5590/api/v1/configuration/\u003cComponentId\u003e/DataFilters\" \"http:  localhost:5590 api v1 configuration \u003cComponentId\u003e DataFilters\" Notes: If you installed the adapter to listen on a non-default port, update 5590 to the port number in use. For a list of other REST operations you can perform, like updating or deleting a data filters configuration, see REST URLs . On successful execution, the change that you have made to data filters takes effect immediately during runtime. Data filters schema The full schema definition for the data filters configuration is in the AdapterName_DataFilters_schema.json file located in one of the following folders: Windows: %ProgramFiles%\\OSIsoft\\Adapters\\\u003cAdapterName\u003e\\Schemas Linux: /opt/OSIsoft/Adapters/\u003cAdapterName\u003e/Schemas  opt OSIsoft Adapters \u003cAdapterName\u003e Schemas Data filters parameters The following parameters are available for configuring data filters: Parameter Required Type Description Id Required string Unique identifier for the data filter. Allowed value: any string identifier AbsoluteDeadband Optional double Specifies the absolute change in data value that should cause the current value to pass the filter test. Note: You must specify AbsoluteDeadband or PercentChange . Allowed value: double value representing absolute deadband number Default value: null PercentChange Optional double Specifies the percent change from previous value that should cause the current value to pass the filter test. Note: You must specify AbsoluteDeadband or PercentChange . Allowed value: double value representing percent change Default value: null ExpirationPeriod Optional timespan The length in time that can elapse after an event before automatically sending the next event, regardless of whether the next event passes the filter or not. The expected format is HH:MM:SS.### or SSS.* Allowed value: any timespan Default value: null * Note: For example, \"ExpirationPeriod\": 5:00 and \"ExpirationPeriod\": 300 both specify an expiration period of 5 minutes and 0 seconds. Data filters example [ { \"Id\": \"DuplicateData\", \"AbsoluteDeadband\": 0, \"PercentChange\": null, \"ExpirationPeriod\": \"01:00:00\" } ] REST URLs Relative URL HTTP verb Action api/v1/configuration/ api v1 configuration  ComponentId /DataFilters  DataFilters GET Gets all configured data filters. api/v1/configuration/ api v1 configuration  ComponentId /DataFilters  DataFilters DELETE Deletes all configured data filters. api/v1/configuration/ api v1 configuration  ComponentId /DataFilters  DataFilters POST Adds an array of data filters or a single data filter. Fails if any data filter already exists. api/v1/configuration/ api v1 configuration  ComponentId /DataFilters  DataFilters PUT Replaces all data. api/v1/configuration/ api v1 configuration  ComponentId /DataFilters  DataFilters PATCH Allows partial updating of configured data filter. api/v1/configuration/ api v1 configuration  ComponentId /DataFilters/  DataFilters  id GET Gets configured data filter by id . api/v1/configuration/ api v1 configuration  ComponentId /DataFilters/  DataFilters  id DELETE Deletes configured data filter by id . api/v1/configuration/ api v1 configuration  ComponentId /DataFilters/  DataFilters  id PUT Replaces data filter by id . Fails if data filter does not exist. Note: Replace ComponentId with the Id of your adapter component."
                                                                    },
    "content/main/shared-content/configuration/diagnostics-and-metadata.html":  {
                                                                                    "href":  "content/main/shared-content/configuration/diagnostics-and-metadata.html",
                                                                                    "title":  "Diagnostics and metadata",
                                                                                    "keywords":  "Diagnostics and metadata You can configure PI adapters to produce and store diagnostics data at a designated health endpoint, and to send metadata for created streams. For more information about available diagnostics data, see Adapter diagnostics and Egress diagnostics . For more information about available metadata and what metadata are sent per metadata level, see Adapter Metadata . Configure general Start any of the Configuration tools capable of making HTTP requests. Run a PUT command to the following endpoint, setting EnableDiagnostics to either true or false , MetadataLevel to None , Low , Medium , or High and HealthPrefix to a string or null : http://localhost:5590/api/v1/configuration/system/general http:  localhost:5590 api v1 configuration system general Note: 5590 is the default port number. If you selected a different port number, replace it with that value. Example using curl : curl -d \"{ \\\"EnableDiagnostics\\\":true, \\\"MetadataLevel\\\":Medium, \\\"HealthPrefix\\\":\\\"Machine1\\\" }\" -X PUT \"http://localhost:5590/api/v1/configuration/system/general\" \"http:  localhost:5590 api v1 configuration system general\" General schema The full schema definition for the general configuration is in the System_General_schema.json file located in one of the following folders: Windows: %ProgramFiles%\\OSIsoft\\Adapters\\\u003cAdapterName\u003e\\Schemas Linux: /opt/OSIsoft/Adapters/\u003cAdapterName\u003e/Schemas  opt OSIsoft Adapters \u003cAdapterName\u003e Schemas General parameters The following parameters are available for configuring general: Parameter Required Type Description EnableDiagnostics Optional boolean Determines if diagnostics are enabled Allowed value: true or false Default value: true MetadataLevel Optional reference Defines amount of metadata sent to OMF endpoints. Allowed value: None , Low , Medium , and High Default value: Medium HealthPrefix Optional reference Prefix to use for health and diagnostics stream and asset IDs. Default value: null Example Retrieve the general configuration Example using curl : curl -X GET \"http://localhost:{port}/api/v1/configuration/system/general\" \"http:  localhost:{port} api v1 configuration system general\" Sample output: { \"EnableDiagnostics\": true, \"MetadataLevel\": \"Medium\", \"HealthPrefix\": \"Machine1\" } REST URLs Relative URL HTTP verb Action api/v1/configuration/system/General api v1 configuration system General GET Gets the general configuration api/v1/configuration/system/General api v1 configuration system General PUT Replaces the existing general configuration api/v1/configuration/system/General api v1 configuration system General PATCH Allows partial updating of general configuration"
                                                                                },
    "content/main/shared-content/configuration/discovery.html":  {
                                                                     "href":  "content/main/shared-content/configuration/discovery.html",
                                                                     "title":  "Discovery",
                                                                     "keywords":  "Discovery You can perform a data discovery for existing data items on demand. Data discovery is initiated through REST calls and it is tied to a specific discovery Id, which you can either specify or let the adapter generate it. Data discovery includes different routes. For example, you can choose to do the following: Retrieve the discovery results Query the discovery status Cancel or delete discoveries Merge discovery results with the data selection configuration Retrieve results from a current discovery and compare it with results from a previous or discovery Retrieve results from a current discovery and compare it with results from a current data selection configuration Configure discovery Start any of the Configuration tools capable of making HTTP requests. Run a POST command with the Id of the discovery and autoSelect set to either true or false to the following endpoint: http://localhost:5590/api/v1/configuration/\u003cComponentId\u003e/Discoveries http:  localhost:5590 api v1 configuration \u003cComponentId\u003e Discoveries . Notes: Including an Id is optional. If you do not include one, the adapter automatically generates one. 5590 is the default port number. If you selected a different port number, replace it with that value. Example using curl : curl -d \"{ \\\"Id\\\":\\\"TestDiscovery\\\", \\\"autoSelect\\\":true }\" -H \"Content-Type:application/json\" \"Content-Type:application json\" -X POST \"http://localhost:5590/api/v1/configuration/\u003cComponentId\u003e/Discoveries\" \"http:  localhost:5590 api v1 configuration \u003cComponentId\u003e Discoveries\" Discovery parameters Parameter Type Description id string The Id of the discovery Notes: ??? You cannot run multiple discoveries with the same Id. ??? Including an id is optional. If you do not include one, the adapter automatically generates one. query string A filter that is specific to the data source. The query filter can limit the scope of the discovery. For more information, see the Data source configuration topic. startTime datetime Time when the discovery started endTime datetime Time when the discovery ended progress double Progress of the discovery itemsFound integer Number of data items that the discovery found on the data source newItems integer Number of new data items that the discovery found in comparison to the previous discovery resultUri integer URL at which you can access the results of the discovery autoSelect boolean When set to true , the result of the discovery gets pushed to the data selection configuration. status reference Status of the discovery, for example Active or Complete errors string Errors encountered during the discovery Discoveries status example The following example shows the status of all discoveries. The discovery id in this example was auto-generated. [ { \"id\": \"8ff855f1-a636-490a-bb31-207410a6e607\", \"query\": null, \"startTime\": \"2020-09-30T19:34:01.8180401+02:00\", \"endTime\": \"2020-09-30T19:34:01.8368776+02:00\", \"progress\": 30, \"itemsFound\": 4, \"newItems\": 0, \"resultUri\": \"http://127.0.0.1:5590/api/v1/Configuration/\u003cComponentId\u003e/Discoveries/8ff855f1-a636-490a-bb31-207410a6e607/result\", \"http:  127.0.0.1:5590 api v1 Configuration \u003cComponentId\u003e Discoveries 8ff855f1-a636-490a-bb31-207410a6e607 result\", \"autoSelect\": false, \"status\": \"Complete\", \"errors\": null } ] REST URLs Relative URL HTTP verb Action api/v1/configuration/ api v1 configuration  \u003ccomponentId\u003e /discoveries  discoveries GET Returns status of all discoveries api/v1/configuration/ api v1 configuration  \u003ccomponentId\u003e /discoveries  discoveries POST Initiates a new discovery and returns its Id api/v1/configuration/ api v1 configuration  \u003ccomponentId\u003e /discoveries  discoveries DELETE Cancels and deletes all saved discoveries api/v1/configuration/ api v1 configuration  \u003ccomponentId\u003e /discoveries/  discoveries  \u003cdiscoveryId\u003e GET Gets the status of an individual discovery Note: If a discovery with the specified Id does not exist, you will get an error message api/v1/configuration/ api v1 configuration  \u003ccomponentId\u003e /discoveries/  discoveries  \u003cdiscoveryId\u003e DELETE Cancels and deletes discovery and result api/v1/configuration/ api v1 configuration  \u003ccomponentId\u003e /discoveries/  discoveries  \u003cdiscoveryId\u003e /result  result GET Returns the result of a discovery api/v1/configuration/ api v1 configuration  \u003ccomponentId\u003e /discoveries/  discoveries  \u003cdiscoveryId\u003e /result?diff=  result?diff= previousId GET Returns the difference between the result and the previous result api/v1/configuration/ api v1 configuration  \u003ccomponentId\u003e /dataselection?diff=  dataselection?diff= \u003cdiscoveryId\u003e GET Returns the difference between the data selection configuration and the discovery results api/v1/configuration/ api v1 configuration  \u003ccomponentId\u003e /discoveries/  discoveries  \u003cdiscoveryId\u003e /result  result DELETE Cancels and deletes discovery result Note: The discovery Id is still valid, but a query will contain a status of canceled Only the Status property will contain a canceled status, but not the query api/v1/configuration/ api v1 configuration  \u003ccomponentId\u003e /discoveries/  discoveries  \u003cdiscoveryId\u003e /cancel  cancel POST Cancels the on-demand data source discovery api/v1/configuration/ api v1 configuration  \u003ccomponentId\u003e /dataselection/select?discoveryid=  dataselection select?discoveryid= \u003cdiscoveryId\u003e POST Adds the discovered items to data selection with selected set to true api/v1/configuration/ api v1 configuration  \u003ccomponentId\u003e /dataselection/unselect?discoveryid=  dataselection unselect?discoveryid= \u003cdiscoveryId\u003e POST Adds the discovered items to data selection with selected set to false Note: Replace \u003ccomponentId\u003e with the Id of your adapter component. Replace \u003cdiscoveryId\u003e with the Id of the discovery for which you want to perform the action."
                                                                 },
    "content/main/shared-content/configuration/egress-endpoints.html":  {
                                                                            "href":  "content/main/shared-content/configuration/egress-endpoints.html",
                                                                            "title":  "Egress endpoints",
                                                                            "keywords":  "Egress endpoints PI adapters collect time series data, which they can send to a permanent data store (endpoint). This operation is called data egress. The following endpoints are available for data egress: OSIsoft Cloud Services (OCS) PI servers through PI Web API For long term storage and analysis, you can configure any adapter to send time series data to one or several of these endpoints in any combination. An egress endpoint is comprised of the properties specified under Egress endpoint parameters . Data egress to a PI server creates a PI point in the PI adapter configuration. Data egress to OCS creates a stream in the PI adapter configuration. The name of the PI point or OCS stream is a combination of the StreamIdPrefix specified in the adapter data source configuration and the StreamId specified in the adapter data selection configuration. Configure egress endpoints Complete the following steps to configure egress endpoints. Use the PUT method in conjunction with the http://localhost:5590/api/v1/configuration/OmfEgress/dataendpoints http:  localhost:5590 api v1 configuration OmfEgress dataendpoints REST endpoint to initialize the configuration. Using a text editor, create an empty text file. Copy and paste an example configuration for egress endpoints into the file. For sample JSON, see Examples . Update the example JSON parameters for your environment. For a table of all available parameters, see Egress endpoint parameters . Save the file. For example, as ConfigureEgressEndpoints.json . Open a command line session. Change directory to the location of ConfigureEgressEndpoints.json . Enter the following cURL command (which uses the PUT method) to initialize the egress endpoints configuration. curl -d \"@ConfigureEgressEndpoints.json\" -H \"Content-Type: application/json\" application json\" -X PUT \"http://localhost:5590/api/v1/configuration/OmfEgress/dataendpoints\" \"http:  localhost:5590 api v1 configuration OmfEgress dataendpoints\" Notes: If you installed the adapter to listen on a non-default port, update 5590 to the port number in use. For a list of other REST operations you can perform, like updating or replacing an egress endpoints configuration, see REST URLs . Egress endpoint configuration schema The full schema definition for the egress endpoint configuration is in the OmfEgress_DataEndpoints_schema.json file located in one of the following folders: Windows: %ProgramFiles%\\OSIsoft\\Adapters\\\u003cAdapterName\u003e\\Schemas Linux: /opt/OSIsoft/Adapters/\u003cAdapterName\u003e/Schemas  opt OSIsoft Adapters \u003cAdapterName\u003e Schemas Egress endpoint parameters The following parameters are available for configuring egress endpoints: Parameter Required Type Description Id Optional string Unique identifier Allowed value: any string identifier Default value: new GUID Endpoint Required string Destination that accepts OMF v1.2 messages. Supported destinations include OCS and PI Server. Allowed value: well-formed http or https endpoint string Default: null Username Required for PI server endpoint string Basic authentication to the PI Web API OMF endpoint PI server: Allowed value: any string Default: null Note: If your username contains a backslash, you must add an escape character, for example, type OilCompany\\TestUser as OilCompany\\\\TestUser . Password Required for PI server endpoint string Basic authentication to the PI Web API OMF endpoint PI server: Allowed value: any string Default: null ClientId Required for OCS endpoint string Authentication with the OCS OMF endpoint Allowed value: any string, can be null if the endpoint URL schema is HTTP Default: null ClientSecret Required for OCS endpoint string Authentication with the OCS OMF endpoint Allowed value: any string, can be null if the endpoint URL schema is HTTP Default: null TokenEndpoint Optional for OCS endpoint string Retrieves an OCS token from an alternative endpoint Allowed value: well-formed http or https endpoint string Default value: null ValidateEndpointCertificate Optional boolean Disables verification of destination certificate. Note: Only use for testing with self-signed certificates. Allowed value: true or false Default value: true Special characters encoding The adapter encodes special characters used in the data selection StreamId parameter string before sending it to configured endpoints. The encoded characters look as follows: Special character Encoded character * %2a \u0027 %27 ` %60 \" %22 ? %3f ; %3b | %7c \\ %5c { %7b } %7d [ %5b ] %5d Examples The following examples are valid egress configurations: Egress data to OCS [{ \"Id\": \"OCS\", \"Endpoint\": \"https://\u003cOCS \"https:  \u003cOCS OMF endpoint\u003e\", \"ClientId\": \"\u003cclientid\u003e\", \"ClientSecret\": \"\u003cclientsecret\u003e\" }] Egress data to PI Web API [{ \"Id\": \"PI Web API\", \"Endpoint\": \"https://\u003cpi \"https:  \u003cpi web api server\u003e:\u003cport\u003e/piwebapi/omf/\", server\u003e:\u003cport\u003e piwebapi omf \", \"UserName\": \"\u003cusername\u003e\", \"Password\": \"\u003cpassword\u003e\" }] REST URLs Relative URL HTTP verb Action api/v1/configuration/omfegress/DataEndpoints api v1 configuration omfegress DataEndpoints GET Gets all configured egress endpoints api/v1/configuration/omfegress/DataEndpoints api v1 configuration omfegress DataEndpoints DELETE Deletes all configured egress endpoints api/v1/configuration/omfegress/DataEndpoints api v1 configuration omfegress DataEndpoints POST Adds an array of egress endpoints or a single endpoint. Fails if any endpoint already exists api/v1/configuration/omfegress/DataEndpoints api v1 configuration omfegress DataEndpoints PUT Replaces all egress endpoints api/v1/configuration/omfegress/DataEndpoints api v1 configuration omfegress DataEndpoints PATCH Allows partial updating of configured endpoints. Note: The request must be an array containing one or more endpoints. Each endpoint in the array must include its Id . api/v1/configuration/omfegress/DataEndpoints/{Id} api v1 configuration omfegress DataEndpoints {Id} GET Gets configured endpoint by Id api/v1/configuration/omfegress/DataEndpoints/{Id} api v1 configuration omfegress DataEndpoints {Id} DELETE Deletes configured endpoint by Id api/v1/configuration/omfegress/DataEndpoints/{Id} api v1 configuration omfegress DataEndpoints {Id} PUT Updates or creates a new endpoint with the specified Id api/v1/configuration/omfegress/DataEndpoints/{Id} api v1 configuration omfegress DataEndpoints {Id} PATCH Allows partial updating of configured endpoint by Id Egress execution details After configuring an egress endpoint, egress is immediately run for that endpoint. Egress is handled individually per configured endpoint. When data is egressed for the first time, types and containers are egressed to the configured endpoint. After that only new or changed types or containers are egressed. Type creation must be successful in order to create containers. Container creation must be successful in order to egress data. If you delete an egress endpoint, data flow immediately stops for that endpoint. Buffered data in a deleted endpoint is permanently lost. Type, container, and data items are batched into one or more OMF messages when egressing. As per the requirements defined in OMF, a single message payload will not exceed 192KB in size. Compression is automatically applied to outbound egress messages. On the egress destination, failure to add a single item results in the message failing. Types, containers, and data are egressed as long as the destination continues to respond to HTTP requests."
                                                                        },
    "content/main/shared-content/configuration/egress-endpoints/configure-a-network-proxy.html":  {
                                                                                                      "href":  "content/main/shared-content/configuration/egress-endpoints/configure-a-network-proxy.html",
                                                                                                      "title":  "Configure a network proxy",
                                                                                                      "keywords":  "Configure a network proxy Some network architectures may need a network proxy between the PI adapter and the egress endpoint. The process for configuring the adapter to egress data through a network proxy varies depending on the proxy type. HTTPS forward proxy For the adapter to use an HTTPS forward proxy while egressing, configure the https_proxy environment variable. For information on how to configure system environment variables, refer to your platform specific documentation: Windows: setx Ubuntu: EnvironmentVariables Debian: EnvironmentVariables Docker: Environment variables in Compose The value of this environment variable must contain the URL of the proxy server, beginning with http . The format of the string is [user[:password]@]http://hostname[:port] [user[:password]@]http:  hostname[:port] . HTTPS proxy environment variable Parameter Required Description user Optional The user name for the HTTPS forward proxy. password Optional The password for the HTTPS forward proxy specified user name. If you specify user , password remains optional. port Optional If you do not specify port , the default 80 is used. Note: Usage of the https_proxy environment variable may affect other .NET or .NET Core applications. If you set this environment variable, it will affect the adapter egress endpoints and the adapter health endpoints. Examples: myUser@http://192.168.2.2 myUser@http:  192.168.2.2 myUser:myPassword@http://proxymachine.domain:3128 myUser:myPassword@http:  proxymachine.domain:3128 http://proxymachine.domain http:  proxymachine.domain In Windows, this may look something like: Example of an architecture with an https forward proxy: Reverse proxy For the adapter to use a reverse proxy while egressing, you must configure the reverse proxy as an egress endpoint. For information on how to configure an egress endpoint, see Egress endpoints configuration . Example: [{ \"Id\": \"PI Web API Through Proxy\", \"Endpoint\": \"https://\u003creverseProxy\u003e:\u003cport\u003e/piwebapi/omf/\", \"https:  \u003creverseProxy\u003e:\u003cport\u003e piwebapi omf \", \"UserName\": \"\u003cpiWebApiUser\u003e\", \"Password\": \"\u003cpiWebApiPassword\u003e\" }] Example of an architecture with a reverse proxy:"
                                                                                                  },
    "content/main/shared-content/configuration/egress-endpoints/prepare-egress-destinations.html":  {
                                                                                                        "href":  "content/main/shared-content/configuration/egress-endpoints/prepare-egress-destinations.html",
                                                                                                        "title":  "Prepare egress destinations",
                                                                                                        "keywords":  "Prepare egress destinations OCS and PI Server destinations may require additional configuration to receive OMF messages. OCS To prepare OCS to receive OMF messages from the adapter, create an OMF connection in OCS. Creating an OMF connection results in an available OMF endpoint that can be used by the adapter egress mechanism. Complete the following steps to create an OMF connection: Create a Client . The Client Id and Client Secret will be used for the corresponding properties in the egress configuration. Create an OMF type Connection . The connection should link the created client to an existing namespace where the data will be stored. The OMF Endpoint URL for the connection will be used as the egress configuration Endpoint property. PI Server To prepare a PI Server to receive OMF messages from the adapter, a PI Web API OMF endpoint must be available. Complete the following steps: Install PI Web API and enable the Open Message Format (OMF) Services feature. During configuration, choose an AF database and PI Data Archive where metadata and data will be stored. The account used in an egress configuration needs permissions to create AF elements, element templates, and PI points. Configure PI Web API to use Basic authentication. For complete steps, as well as best practices and recommendations, see the following topic in the PI Web API User Guide: Authentication methods . Notes: The certificate used by PI Web API must be trusted by the device running the adapter, otherwise the egress configuration ValidateEndpointCertificate property needs to be set to false (this can be the case with a self-signed certificate but should only be used for testing purposes). To continue to send OMF egress messages to the PI Web API endpoint after upgrading PI Web API, restart the adapter service."
                                                                                                    },
    "content/main/shared-content/configuration/health-endpoints.html":  {
                                                                            "href":  "content/main/shared-content/configuration/health-endpoints.html",
                                                                            "title":  "Health endpoints",
                                                                            "keywords":  "Health endpoints You can configure PI adapters to produce and store health data at a designated health endpoint. You can use health data to ensure that your adapters are running properly and that data flows to the configured OMF endpoints. For more information about adapter health, see Adapter health . Configure health endpoint A health endpoint designates an OMF endpoint where adapter health information is sent. You can configure multiple health endpoints. Complete the following steps to configure health endpoints. Use the PUT method in conjunction with the http://localhost:5590/api/v1/configuration/system/healthendpoints http:  localhost:5590 api v1 configuration system healthendpoints REST endpoint to initialize the configuration. Using a text editor, create an empty text file. Copy and paste an example configuration for health endpoints into the file. For sample JSON, see Examples . Update the example JSON parameters for your environment. For a table of all available parameters, see Health endpoint parameters . Save the file. For example, as ConfigureHealthEndpoints.json . Open a command line session. Change directory to the location of ConfigureHealthEndpoints.json . Enter the following cURL command (which uses the PUT method) to initialize the health endpoint configuration. curl -d \"@ConfigureHealthEndpoints.json\" -H \"Content-Type: application/json\" application json\" -X PUT \"http://localhost:5590/api/v1/configuration/system/healthendpoints\" \"http:  localhost:5590 api v1 configuration system healthendpoints\" Notes: If you installed the adapter to listen on a non-default port, update 5590 to the port number in use. For a list of other REST operations you can perform, like updating or replacing a health endpoints configuration, see REST URLs . Health endpoints schema The full schema definition for the health endpoint configuration is in the System_HealthEndpoints_schema.json file located in one of the following folders: Windows: %ProgramFiles%\\OSIsoft\\Adapters\\\u003cAdapterName\u003e\\Schemas Linux: /opt/OSIsoft/Adapters/\u003cAdapterName\u003e/Schemas  opt OSIsoft Adapters \u003cAdapterName\u003e Schemas Health endpoint parameters The following parameters are available for configuring health endpoints: Parameter Required Type Description Id Optional string Uniquely identifies the endpoint. This can be any alphanumeric string. If left blank, a unique value is generated automatically. Allowed value: any string identifier Default value: new GUID Endpoint Required string The URL of the OMF endpoint to receive this health data Allowed value: well-formed http or https endpoint string Default: null Username Required for PI Web API endpoints string The username used to authenticate with a PI Web API OMF endpoint PI server: Allowed value: any string Default: null Password Required for PI Web API endpoints string The password used to authenticate with a PI Web API OMF endpoint PI server: Allowed value: any string Default: null ClientId Required for OCS endpoints string The client ID used for authentication with an OSIsoft Cloud Services OMF endpoint Allowed value: any string Default: null ClientSecret Required for OCS endpoints string The client secret used for authentication with an OSIsoft Cloud Services OMF endpoint Allowed value: any string Default: null TokenEndpoint Optional for OCS endpoints string Retrieves an OCS token from an alternative endpoint Allowed value: well-formed http or https endpoint string Default value: null ValidateEndpointCertificate Optional boolean Disables verification of destination security certificate. Use for testing only with self-signed certificates; OSIsoft recommends keeping this set to the default, true, in production environments. Allowed value: true or false Default value: true Examples OCS endpoint { \"Id\": \"OCS\", \"Endpoint\": \"https://\u003cOCS \"https:  \u003cOCS OMF endpoint\u003e\", \"ClientId\": \"\u003cclientid\u003e\", \"ClientSecret\": \"\u003cclientsecret\u003e\" } PI Web API endpoint { \"Id\": \"PI Web API\", \"Endpoint\": \"https://\u003cpi \"https:  \u003cpi web api server\u003e:\u003cport\u003e/piwebapi/omf/\", server\u003e:\u003cport\u003e piwebapi omf \", \"UserName\": \"\u003cusername\u003e\", \"Password\": \"\u003cpassword\u003e\" } Note: When you use an adapter with a PI Web API health endpoint, the AF structure is required. If the elements are deleted, the adapter recreates the elements; if the account used to authenticate to the PI Web API has its permissions removed on the AF Server, the adapter retries sending health data to the PI Web API until the permissions are restored. REST URLs Relative URL HTTP verb Action api/v1/configuration/system/healthEndpoints api v1 configuration system healthEndpoints GET Gets all configured health endpoints api/v1/configuration/system/healthEndpoints api v1 configuration system healthEndpoints DELETE Deletes all configured health endpoints api/v1/configuration/system/healthEndpoints api v1 configuration system healthEndpoints POST Adds an array of health endpoints or a single endpoint. Fails if any endpoint already exists api/v1/configuration/system/healthEndpoints api v1 configuration system healthEndpoints PUT Replaces all health endpoints. Note: Requires an array of endpoints api/v1/configuration/system/healthEndpoints api v1 configuration system healthEndpoints PATCH Allows partial updating of configured health endpoints Note: The request must be an array containing one or more health endpoints. Each health endpoint in the array must include its Id . api/v1/configuration/system/healthEndpoints/ api v1 configuration system healthEndpoints  Id GET Gets configured health endpoint by Id api/v1/configuration/system/healthEndpoints/ api v1 configuration system healthEndpoints  Id DELETE Deletes configured health endpoint by Id api/v1/configuration/system/healthEndpoints/ api v1 configuration system healthEndpoints  Id PUT Updates or creates a new health endpoint with the specified Id api/v1/configuration/system/healthEndpoints/ api v1 configuration system healthEndpoints  Id PATCH Allows partial updating of configured health endpoint by Id Note: Replace Id with the Id of the health endpoint."
                                                                        },
    "content/main/shared-content/configuration/history-recovery.html":  {
                                                                            "href":  "content/main/shared-content/configuration/history-recovery.html",
                                                                            "title":  "History recovery",
                                                                            "keywords":  "History recovery The adapter you are using supports the following data collection modes which you configure in the DataCollectionMode parameter of your adapter\u0027s data source configuration: CurrentOnly : The adapter component operates normally. History recovery is disabled. CurrentWithBackfill (Default): The adapter component operates normally, but disconnections and shutdown events are recorded in the form of recovery intervals. When the adapter is reconnected to a data source, it automatically backfills data for the recorded intervals. HistoryOnly : The adapter component does not get started. The adapter is able to start collecting historical data on demand. History recovery for adapters supports the following two operations related to the data collection mode: On demand history recovery : Recovers data from a specified start time or start and end time. If end time is not specified, the default is utcnow . On demand history recovery is available only when the adapter is in HistoryOnly data collection mode. Limited automatic history recovery : Backfills data gaps that originated from connection disruptions, data source issues, or PI adapter shutdown or both. This is limited to a maximum time-range of four days. Limited automatic history recovery is available only when the adapter is in CurrentWithBackfill data collection mode."
                                                                        },
    "content/main/shared-content/configuration/history-recovery/automatic-history-recovery.html":  {
                                                                                                       "href":  "content/main/shared-content/configuration/history-recovery/automatic-history-recovery.html",
                                                                                                       "title":  "Automatic history recovery",
                                                                                                       "keywords":  "Automatic history recovery Besides on-demand history recovery, the PI adapter also supports automatic history recovery. For automatic history recovery, the adapter tracks changes to the DeviceStatus of each component. When the DeviceStatus changes to DeviceInError or Shutdown , the adapter starts a new History recovery interval . When the issue resolves or if the adapter is restarted and the DeviceStatus changes to Good , the adapter closes any current intervals for that component. The adapter tracks these intervals for each component and, when DeviceStatus has a value of Good , it performs history recovery for these intervals starting from oldest to newest. For more information, see also Device status . Note: If the data collection mode is set to CurrentWithBackfill , the adapter clears periods not recovered for the component and stops keeping track of them. Only if the data collection mode is set to HistoryOnly , an automatic history recovery operation in progress will be canceled, otherwise it will be finished. History recovery intervals Automatic history intervals cannot be longer than four days. If an interval is longer than four days, the adapter automatically changes the start time of the interval to be no earlier than four days before the end time prior to starting a recovery. If a current outage lasts longer than four days, when the device status finally improves the adapter recovers up to four days before the current time. This avoids introducing additional data gaps."
                                                                                                   },
    "content/main/shared-content/configuration/history-recovery/on-demand-history-recovery-configuration.html":  {
                                                                                                                     "href":  "content/main/shared-content/configuration/history-recovery/on-demand-history-recovery-configuration.html",
                                                                                                                     "title":  "On-demand history recovery configuration",
                                                                                                                     "keywords":  "On-demand history recovery configuration The PI adapter supports performing history recovery on-demand by specifying start and end time. Configure history recovery Start any of the Configuration tools capable of making HTTP requests. Run a POST command with the Id of the history recovery, and the startTime and endTime to the following endpoint: http://localhost:5590/api/v1/configuration/\u003cComponentId\u003e/HistoryRecoveries http:  localhost:5590 api v1 configuration \u003cComponentId\u003e HistoryRecoveries . Example using curl : curl -d \"{ \\\"Id\\\":\\\"TestRecovery\\\", \\\"startTime\\\":\\\"2021-03-29T14:00:30Z\\\", \\\"endTime\\\":\\\"2021-03-29T15:00:15Z\\\" }\" -X POST \"http://localhost:5590/api/v1/configuration/\u003cComponentId\u003e/HistoryRecoveries\" \"http:  localhost:5590 api v1 configuration \u003cComponentId\u003e HistoryRecoveries\" Note: 5590 is the default port number. If you selected a different port number, replace it with that value. If you do not specify an Id, the endpoint generates a unique Id. History recovery parameters Parameter Type Description Id string The Id of the history recovery Note: You cannot run multiple history recoveries with the same Id. StartTime datetime Time when the the first data items are collected. EndTime datetime Time when the last data items are collected. Checkpoint datetime The latest timestamp that the history recovery has completed with the range being between startTime and endTime . Items double The amount of data selection items in the history recovery operation. RecoveredEvents double Number of events that the history recovery found on the data source. Progress double Progress of the history recovery (number of data items found through the history recovery). Status enum Status of the history recovery. The following statuses are available: - Active - The operation is currently in progress - Complete - The operation has been completed - Canceled - The operation has been canceled - Failed - The operation failed Errors string Errors encountered during the history recovery. History recovery status example [ { \"Id\": \"HistoryRecovery1\", \"StartTime\": \"2021-01-09T05:55:00.0\", \"EndTime\": \"2021-01-26T13:20:00.0\", \"CheckPoint\": \"2021-01-13T14:55:00.0\", \"Items\": 7000, \"RecoveredEvents\": 800000, \"Progress\": 20, \"Status\": \"Active\", \"Errors\": null } ] Note: The result of the history recovery operation is added to the \u003ccomponentId\u003e_historyRecoveries.json file. REST URLs Relative URL HTTP verb Action api/v1/configuration/ api v1 configuration  \u003ccomponentId\u003e /historyRecoveries  historyRecoveries GET Returns all history recoveries statuses api/v1/configuration/ api v1 configuration  \u003ccomponentId\u003e /historyRecoveries  historyRecoveries POST Initiates a new history recovery, returns the id of the operation api/v1/configuration/ api v1 configuration  \u003ccomponentId\u003e /historyRecoveries  historyRecoveries DELETE Cancels all active history recovery operations and removes states api/v1/configuration/ api v1 configuration  \u003ccomponentId\u003e /historyRecoveries/  historyRecoveries  \u003coperationId\u003e GET Gets the status of an individual history recovery api/v1/configuration/ api v1 configuration  \u003ccomponentId\u003e /historyRecoveries/  historyRecoveries  \u003coperationId\u003e DELETE Cancels history recovery and removes the state api/v1/configuration/ api v1 configuration  \u003ccomponentId\u003e /historyRecoveries/  historyRecoveries  \u003coperationId\u003e /cancel  cancel POST Cancels history recovery api/v1/configuration/ api v1 configuration  \u003ccomponentId\u003e /historyRecoveries/  historyRecoveries  \u003coperationId\u003e /resume  resume POST Resumes canceled or failed history recovery operation ( 202 ) from the checkpoint Note: If the \u003coperationId\u003e is not found, a 404 HTTP error message will be returned Note: Replace \u003ccomponentId\u003e with the Id of your adapter component. Replace \u003coperationId\u003e with the Id of the history recovery operation for which you want to perform the action."
                                                                                                                 },
    "content/main/shared-content/configuration/logging.html":  {
                                                                   "href":  "content/main/shared-content/configuration/logging.html",
                                                                   "title":  "Logging",
                                                                   "keywords":  "Logging PI adapters write daily log messages for the adapter, the system, and OMF egress to flat text files in the following locations: ??? Windows: %ProgramData%\\OSIsoft\\Adapters{AdapterInstance}\\Logs ??? Linux: /usr/share/OSIsoft/Adapters/{AdapterInstance}/Logs  usr share OSIsoft Adapters {AdapterInstance} Logs Each message in the log displays the message severity level, timestamp, and the message itself. Configure logging Complete the following steps to configure logging. Use the PUT method in conjunction with the http://localhost:5590/api/v1/configuration/\u003cComponentId\u003e/Logging http:  localhost:5590 api v1 configuration \u003cComponentId\u003e Logging REST endpoint to initialize the configuration. Using a text editor, create an empty text file. Copy and paste an example configuration for logging into the file. For sample JSON, see Example . Update the example JSON parameters for your environment. For a table of all available parameters, see Logging parameters . Save the file. For example, as ConfigureLogging.json . Open a command line session. Change directory to the location of ConfigureLogging.json . Enter the following cURL command (which uses the PUT method) to initialize the logging configuration. curl -d \"@ConfigureLogging.json\" -H \"Content-Type: application/json\" application json\" -X PUT \"http://localhost:5590/api/v1/configuration/\u003cComponentId\u003e/Logging\" \"http:  localhost:5590 api v1 configuration \u003cComponentId\u003e Logging\" Notes: If you installed the adapter to listen on a non-default port, update 5590 to the port number in use. For a list of other REST operations you can perform, like updating or retrieving a logging configuration, see REST URLs . Any parameter not specified in the updated configuration file reverts to the default schema value On successful execution, the log-level change takes effect immediately during runtime. The other configurations (log file size and file count) are updated after the adapter is restarted. Logging schema The full schema definition for the logging configuration is in the component specific logging file: AdapterName_Logging_schema.json , OmfEgress_Logging_schema.json , or System_Logging_schema.json file located in one of the following folders: Windows: %ProgramFiles%\\OSIsoft\\Adapters\\\u003cAdapterName\u003e\\Schemas Linux: /opt/OSIsoft/Adapters/\u003cAdapterName\u003e/Schemas  opt OSIsoft Adapters \u003cAdapterName\u003e Schemas Logging parameters The following parameters are available for configuring logging: Parameter Required Type Description LogLevel Optional reference The logLevel sets the minimum severity for messages to be included in the logs. Messages with a severity below the level set are not included. The log levels in their increasing order of severity are as follows: Trace , Debug , Information , Warning , Error , Critical , and None . Default log level: Information For detailed information about the log levels, see LogLevel . LogFileSizeLimitBytes Optional integer The maximum size (in bytes) of log files that the service will create for the component. The value must be a positive integer. Minimum value: 1000 Maximum value: 9223372036854775807 Default value: 34636833 LogFileCountLimit Optional integer The maximum number of log files that the service will create for the component. The value must be a positive integer. Minimum value: 1 Maximum value: 2147483647 Default value: 31 LogLevel Level Description Trace Logs that contain the most detailed messages. These messages may contain sensitive application data like actual received values, which is why these messages should not be enabled in production environment. Note: Trace is translated to Verbose in the log file. Debug Logs that can be used to troubleshoot data flow issues by recording metrics and detailed flow related information. Information Logs that track the general flow of the application. Any non-repetitive general information like the following can be useful for diagnosing potential application errors: - Version information related to the software at startup - External services used - Data source connection string - Number of measurements - Egress URL - Change of state ???Starting??? or ???Stopping??? - Configuration Warning Logs that highlight an abnormal or unexpected event in the application flow that does not otherwise cause the application execution to stop. Warning messages can indicate an unconfigured data source state, an insecure communication channel in use, or any other event that could require attention but that does not impact data flow. Error Logs that highlight when the current flow of execution is stopped due to a failure. These should indicate a failure in the current activity and not an application-wide failure. It can indicate an invalid configuration, unavailable external endpoint, internal flow error, and so on. Critical Logs that describe an unrecoverable application or system crash or a catastrophic failure that requires immediate attention. This can indicate application wide failures like beta timeout expired, unable to start self-hosted endpoint, unable to access vital resource (for example, Data Protection key file), and so on. Note: Critical is translated to Fatal in the log file. None Logging is disabled for the given component. Example Default logging configuration By default, logging captures Information, Warning, Error, and Critical messages in the message logs. The following logging configuration is the installation default for a component: { \"logLevel\": \"Information\", \"logFileSizeLimitBytes\": 34636833, \"logFileCountLimit\": 31 } REST URLs Relative URL HTTP verb Action api/v1/configuration/System/Logging api v1 configuration System Logging GET Retrieves the system logging configuration api/v1/configuration/System/Logging api v1 configuration System Logging PUT Updates the system logging configuration api/v1/configuration/ api v1 configuration  ComponentId /Logging  Logging GET Retrieves the logging configuration of the specified adapter component api/v1/configuration/ api v1 configuration  ComponentId /Logging  Logging PUT Updates the logging configuration of the specified adapter component Note: Replace ComponentId with the Id of your adapter component."
                                                               },
    "content/main/shared-content/configuration/schedules.html":  {
                                                                     "href":  "content/main/shared-content/configuration/schedules.html",
                                                                     "title":  "Schedules",
                                                                     "keywords":  "Schedules You can configure the adapter to run scans based on a schedule. Each data item can be assigned to a schedule in the data selection configuration. The adapter samples data for those data items at the scheduled time. Note: You start an ingress component without a schedule configuration, a default schedule configuration is added to use as an example. Note: When the adapter framework scheduler misses or skips a scan for any reason, either one of the following messages is printed: Scan skipped for schedule id \u003cId\u003e or Scan missed for schedule \u003cid\u003e . Configure schedules Complete the following steps to change the schedules configuration: Using any text editor, create a file that contains the schedules configuration in the JSON format. For content structure, see the example schedule configuration . For all available parameters, see the schedules parameters . Save the file. For example, ConfigureSchedules.json . Use any of the Configuration tools capable of making HTTP requests to run a PUT command with the contents of the file to the following endpoint: http://localhost:5590/api/v1/configuration/\u003cComponentId\u003e/Schedules http:  localhost:5590 api v1 configuration \u003cComponentId\u003e Schedules . Note: Replace \u003cComponentId\u003e with the ComponentId of the adapter. 5590 is the default port number. If you selected a different port number, replace it with that value. Example using curl : Note: Run this command from the same directory where the file is located. curl -d \"@ConfigureSchedules.json\" -H \"Content-Type: application/json\" application json\" -X PUT \"http://localhost:5590/api/v1/configuration/\u003cComponentId\u003e/Schedules\" \"http:  localhost:5590 api v1 configuration \u003cComponentId\u003e Schedules\" On successful execution, the schedules change takes effect immediately during runtime. Schedules schema The full schema definition for the schedules configuration is in the AdapterName_Schedules_schema.json file located in one of the following folders: Windows: %ProgramFiles%\\OSIsoft\\Adapters\\\u003cAdapterName\u003e\\Schemas Linux: /opt/OSIsoft/Adapters/\u003cAdapterName\u003e/Schemas  opt OSIsoft Adapters \u003cAdapterName\u003e Schemas Schedules parameters The following parameters are available for configuring schedules: Parameter Required Type Description Id Required string Unique identifier for the schedule Allowed value: any string identifier Period Required string The data sampling rate of the schedule. The expected format is HH:MM:SS.###. Invalid inputs: null , negative timespan, or zero A default value must be specified. Offset Optional string The offset from the midnight when the schedule starts. The expected format is HH:MM:SS.### Invalid input: negative timespan A default value must be specified. Note: You can also specify timespans as numbers in seconds. For example, \"Period\": 25 specifies 25 seconds, or \"Period\": 125 specifies 2 minutes and 5 seconds. Example schedule configuration The following is an example of a complete schedule configuration: [ { \"Id\": \"schedule1\", \"Period\": \"00:00:01.500\", \"Offset\": \"00:02:03\" } ] Default schedule configuration If no schedule is configured, the adapter uses the following default schedule configuration: [ { \"Id\": \"1\", \"Period\": \"0:00:05\", \"Offset\": \"0:00:00\" } ] REST URLs Relative URL HTTP verb Action api/v1/configuration/ api v1 configuration  ComponentId /Schedules  Schedules GET Gets all configured schedules api/v1/configuration/ api v1 configuration  ComponentId /Schedules  Schedules DELETE Deletes all configured schedules api/v1/configuration/ api v1 configuration  ComponentId /Schedules  Schedules POST Adds an array of schedules or a single schedule. Fails if any schedule already exists api/v1/configuration/ api v1 configuration  ComponentId /Schedules  Schedules PUT Replaces all schedules api/v1/configuration/ api v1 configuration  ComponentId /Schedules/  Schedules  id GET Gets configured schedule by id api/v1/configuration/ api v1 configuration  ComponentId /Schedules/  Schedules  id DELETE Deletes configured schedule by id api/v1/configuration/ api v1 configuration  ComponentId /Schedules/  Schedules  id PUT Replaces schedule by id . Fails if schedule does not exist api/v1/configuration/ api v1 configuration  ComponentId /Schedules/  Schedules  id PATCH Allows partial updating of configured schedule by id Note: Replace ComponentId with the Id of your adapter component."
                                                                 },
    "content/main/shared-content/configuration/system-and-adapter.html":  {
                                                                              "href":  "content/main/shared-content/configuration/system-and-adapter.html",
                                                                              "title":  "System and adapter",
                                                                              "keywords":  "System and adapter You can configure the system component and adapter component together using a single file. Change system and adapter configuration Complete the following steps to configure system and adapter. Use the PUT method in conjunction with the http://localhost:5590/api/v1/configuration http:  localhost:5590 api v1 configuration REST endpoint to initialize the configuration. Using a text editor, create an empty text file. Copy and paste an example configuration for system and adapter into the file. For sample JSON, see the corresponding adapter configuration examples topic. Save the file. For example, as ConfigureSystemAndAdapter.json . Open a command line session. Change directory to the location of ConfigureSystemAndAdapter.json . Enter the following cURL command (which uses the PUT method) to initialize the system and adapter configuration. curl -d \"@ConfigureSystemAndAdapter.json\" -H \"Content-Type: application/json\" application json\" -X PUT \"http://localhost:5590/api/v1/configuration\" \"http:  localhost:5590 api v1 configuration\" Notes: If you installed the adapter to listen on a non-default port, update 5590 to the port number in use. In order for some of the adapter specific configurations to take effect, you have to restart the adapter. Discoveries and HistoryRecoveries facet details are not required to be supplied as part of the configuration and supplied values will be ignored. Their results will be restored from the previous states. If the operation fails due to errors in the configuration, the count of the error and suitable error messages are returned in the result. REST URLs Relative URL HTTP verb Action api/v1/configuration/ api v1 configuration  PUT Replaces the configuration for the entire adapter"
                                                                          },
    "content/main/shared-content/configuration/system-components.html":  {
                                                                             "href":  "content/main/shared-content/configuration/system-components.html",
                                                                             "title":  "System components",
                                                                             "keywords":  "System components PI adapters use JSON configuration files in a protected directory on Windows and Linux to store configuration that is read on startup. While the files are accessible to view, OSIsoft recommends that you use REST or the EdgeCmd utility for any changes you make to the files. As part of making adapters as secure as possible, any passwords or secrets that you configure are stored in encrypted form where cryptographic key material is stored separately in a secure location. If you edit the files directly, the adapter may not work as expected. Note: You can edit any single component or facet of the system individually using REST, but you can also configure the system as a whole with a single REST call. Configure system components Complete the following steps to configure system components. Use the PUT method in conjunction with the http://localhost:5590/api/v1/configuration/system/components http:  localhost:5590 api v1 configuration system components REST endpoint to initialize the configuration. Using a text editor, create an empty text file. Copy and paste an example configuration for system components into the file. For sample JSON, see Examples . Update the example JSON parameters for your environment. For a table of all available parameters, see System components parameters . Save the file. For example, as ConfigureComponents.json . Open a command line session. Change directory to the location of ConfigureComponents.json . Enter the following cURL command (which uses the PUT method) to initialize the system components configuration. curl -d \"@ConfigureComponents.json\" -H \"Content-Type: application/json\" application json\" -X PUT \"http://localhost:5590/api/v1/configuration/system/components\" \"http:  localhost:5590 api v1 configuration system components\" Notes: If you installed the adapter to listen on a non-default port, update 5590 to the port number in use. For a list of other REST operations you can perform, like updating or deleting a system components configuration, see REST URLs . System components schema The full schema definition for the system components configuration is in the System_Components_schema.json file located in one of the following folders: Windows: %ProgramFiles%\\OSIsoft\\Adapters\\AdapterName\\Schemas Linux: /opt/OSIsoft/Adapters/AdapterName/Schemas  opt OSIsoft Adapters AdapterName Schemas System components parameters You can configure the following parameters for system components: Parameters Required Type Description ComponentId Required string The ID of the component 1 . It can be any alphanumeric string. A properly configured ComponentID follows these rules: Cannot contain leading or trailing space Cannot use the following characters: \u003e \u003c /   : ? # [ ] @ ! $ \u0026 * \" ( ) \\\\ + , ; = ` ComponentType Required string The type of the component. There are two types of components: OmfEgress and the adapter. 1 1 Note: The OmfEgress component is required to run the adapter. Both its ComponentId and ComponentType are reserved and should not be modified. Examples Default system components configuration The default System_Components.json file for the System component contains the following information. [ { \"ComponentId\": \"OmfEgress\", \"ComponentType\": \"OmfEgress\" } ] System components configuration with two adapter instances [ { \"ComponentId\": \"\u003cAdapterName\u003e1\", \"ComponentType\": \"\u003cAdapterName\u003e\" }, { \"ComponentId\": \"\u003cAdapterName\u003e2\", \"ComponentType\": \"\u003cAdapterName\u003e\" }, { \"ComponentId\": \"OmfEgress\", \"ComponentType\": \"OmfEgress\" } ] REST URLs Relative URL HTTP verb Action api/v1/configuration/system/components api v1 configuration system components GET Retrieves the system components configuration api/v1/configuration/system/components api v1 configuration system components POST Adds a new component to the system configuration api/v1/configuration/system/components api v1 configuration system components PUT Updates the system components configuration api/v1/configuration/system/components/ api v1 configuration system components  ComponentId DELETE Deletes a specific component from the system components configuration api/v1/configuration/system/components/ api v1 configuration system components  ComponentId PUT Creates a new component with the specified ComponentId in the system configuration"
                                                                         },
    "content/main/shared-content/configuration/text-parser/jsonpath-syntax-for-value-retrieval.html":  {
                                                                                                           "href":  "content/main/shared-content/configuration/text-parser/jsonpath-syntax-for-value-retrieval.html",
                                                                                                           "title":  "JSONPath syntax for value retrieval",
                                                                                                           "keywords":  "JSONPath syntax for value retrieval For information on which semantic is used for retrieving values from JSON files, see JSONPath Syntax . The following syntax is used to extract values from JSON documents. JSON - Simple JSONPath example [ { \"time\": \"2020-08-10T12:10:46.0928791Z\", \"value\": 1.234567890 }, { \"time\": \"2020-08-10T12:10:47.0928791Z\", \"value\": 12.34567890 }, { \"time\": \"2020-08-10T12:10:48.0928791Z\", \"value\": 123.4567890 }, { \"time\": \"2020-08-10T12:10:49.0928791Z\", \"value\": 1234.567890 }, { \"time\": \"2020-08-10T12:10:50.0928791Z\", \"value\": 12345.67890 }, { \"time\": \"2020-08-10T12:10:51.0928791Z\", \"value\": 123456.7890 }, { \"time\": \"2020-08-10T12:10:52.0928791Z\", \"value\": 12345678.90 }, { \"time\": \"2020-08-10T12:10:53.0928791Z\", \"value\": 123456789.0 } ] The following JSONPath configuration reads a series of values: { \"Id\": \"DoubleValue\", \"FieldDefinition\": \"value\", \"DataType\": \"Double\" }, { \"Id\": \"Timestamp\", \"FieldDefinition\": \"time\", \"DataType\": \"DateTime\", \"IsIndex\": true } JSON - Complex JSONPath examples The following example reads specific values from a JSON array: { \"StreamData\": { \"TPPrototype.uflsample.value_time\": [ { \"StreamId\": \"TPPrototype.uflsample.value_time\", \"DataType\": \"Double\", \"Timestamp\": \"2013-12-01T06:00:00Z\", \"Value\": 339.0 }, { \"StreamId\": \"TPPrototype.uflsample.value_time\", \"DataType\": \"Double\", \"Timestamp\": \"2013-12-01T07:00:00Z\", \"Value\": 344.0 }, { \"StreamId\": \"TPPrototype.uflsample.value_time\", \"DataType\": \"Double\", \"Timestamp\": \"2013-12-01T17:00:00Z\", \"Value\": 341.0 } ], \"TPPrototype.uflsample.value_timeString\": [ { \"StreamId\": \"TPPrototype.uflsample.value_timeString\", \"DataType\": \"String\", \"Timestamp\": \"2013-12-01T06:00:00Z\", \"Value\": \"339.0\" }, { \"StreamId\": \"TPPrototype.uflsample.value_timeString\", \"DataType\": \"String\", \"Timestamp\": \"2013-12-01T07:00:00Z\", \"Value\": \"344.0\" }, { \"StreamId\": \"TPPrototype.uflsample.value_timeString\", \"DataType\": \"String\", \"Timestamp\": \"2013-12-01T17:00:00Z\", \"Value\": \"341.0\" } ], \"TPPrototype.uflsample.pressure_time\": [ { \"StreamId\": \"TPPrototype.uflsample.pressure_time\", \"DataType\": \"Double\", \"Timestamp\": \"2013-12-01T06:00:00Z\", \"Value\": 339.0 }, { \"StreamId\": \"TPPrototype.uflsample.pressure_time\", \"DataType\": \"Double\", \"Timestamp\": \"2013-12-01T07:00:00Z\", \"Value\": 344.0 }, { \"StreamId\": \"TPPrototype.uflsample.pressure_time\", \"DataType\": \"Double\", \"Timestamp\": \"2013-12-01T17:00:00Z\", \"Value\": 341.0 } ] } } The following JSONPath configuration reads all the TPPrototype.uflsample.value_time values from the JSON above: { \"Id\": \"Value\", \"DataType\": \"Double\", \"FieldDefinition\": \"$[\u0027StreamData\u0027].[\u0027TPPrototype.uflsample.value_time\u0027][*].Value\" }, { \"Id\": \"Time\", \"DataType\": \"DateTime\", \"FieldDefinition\": \"$[\u0027StreamData\u0027].[\u0027TPPrototype.uflsample.value_time\u0027][*].Timestamp\", \"IsIndex\": true } The following example reads specific value from complex nested JSON: { \"success\": true, \"error\": null, \"result\": { \"type\": \"runtime_history\", \"chart\": { \"chart\": { \"type\": \"column\" }, \"title\": { \"text\": \"\" }, \"subtitle\": { \"text\": \"Daily History\" }, \"colors\": [ \"#fee292\", \"#fdc152\", \"#f69638\", \"#f17130\", \"#9f2d26\", \"#8acadc\", \"#184c8e\" ], \"series\": [ { \"name\": \"Stage 3 Aux Heat\", \"data\": [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ], \"stack\": \"heat\", \"state\": \"heat_aux_stage3\" }, { \"name\": \"Stage 2 Aux Heat\", \"data\": [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ], \"stack\": \"heat\", \"state\": \"heat_aux_stage2\" }, { \"name\": \"Aux Heat\", \"data\": [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ], \"stack\": \"heat\", \"state\": \"heat_aux\" }, { \"name\": \"Stage 2 Heat\", \"data\": [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ], \"stack\": \"heat\", \"state\": \"heat_stage2\" }, { \"name\": \"Heat\", \"data\": [ 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.3, 0.2, 0.0 ], \"stack\": \"heat\", \"state\": \"heat\" }, { \"name\": \"Stage 2 Cool\", \"data\": [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ], \"stack\": \"cool\", \"state\": \"cool_stage2\" }, { \"name\": \"Cool\", \"data\": [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ], \"stack\": \"cool\", \"state\": \"cool\" } ], \"xAxis\": { \"categories\": [ \"Friday\", \"Saturday\", \"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\" ], \"labels\": { \"rotation\": -45 } }, \"yAxis\": { \"allowDecimals\": false, \"min\": 0, \"max\": 24, \"tickInternval\": 4, \"title\": { \"text\": \"Runtime (Hours)\" } }, \"legend\": { \"layout\": \"vertical\", \"align\": \"center\", \"floating\": false, \"shadow\": false, \"itemStyle\": { \"fontSize\": \"1em\" } }, \"tooltip\": { \"shared\": true, \"borderColor\": \"#000000\" }, \"credits\": { \"enabled\": false }, \"plotOptions\": { \"column\": { \"stacking\": \"normal\" }, \"series\": { \"shadow\": false } } }, \"table\": { \"headings\": [ \"Fri\", \"Sat\", \"Sun\", \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\" ], \"series\": [ { \"name\": \"Aux Heat\", \"data\": [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ], \"stack\": \"heat\", \"state\": \"heat_aux\" }, { \"name\": \"Outdoor High Temp.\", \"data\": [ 72.0, 64.0, 73.0, 72.0, 67.0, 73.0, 77.0, 62.0, 51.0 ], \"stack\": null, \"state\": \"outdoor_high_temperature\" }, { \"name\": \"Outdoor Low Temp.\", \"data\": [ 55.0, 60.0, 62.0, 61.0, 51.0, 43.0, 46.0, 44.0, 35.0 ], \"stack\": null, \"state\": \"outdoor_low_temperature\" }, { \"name\": \"Avg Indoor Temp.\", \"data\": [ 76.0, 77.0, 78.0, 78.0, 77.0, 73.0, 74.0, 75.0, 72.0 ], \"stack\": null, \"state\": \"average_indoor_temperature\" }, { \"name\": \"Avg Indoor Humidity\", \"data\": [ 66.0, 68.0, 70.0, 70.0, 69.0, 67.0, 67.0, 66.0, 61.0 ], \"stack\": null, \"state\": \"average_indoor_humidity\" }, { \"name\": \"Fan Only Runtime\", \"data\": [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ], \"stack\": null, \"state\": \"fan_only\" }, { \"name\": \"Vent\", \"data\": [], \"stack\": null, \"state\": \"vent\" } ] }, \"show_monthly_runtime_history\": true } } The following JSONPath configuration reads Sunday Average Indoor Temperature. The timestamp comes from Adapter local time. { \"Id\": \"Temperature\", \"DataType\": \"Double\", \"FieldDefinition\": \"$.result.table.series[3].data[2]\" }, { \"Id\": \"Timestamp\", \"DataType\": \"DateTime\", \"Format\": \"Adapter\", \"IsIndex\": true } Error handling If you encounter text parser related errors that is errors for the ValueField or IndexField , check the StreamId associated with the error message. Possible errors include the following: The JSONPath expression of ValueField or IndexField is pointing to a non-existing value The JSONPath expression of ValueField or IndexField is missing a value altogether DataType does not match the value"
                                                                                                       },
    "content/main/shared-content/configuration/text-parser/text-parser.html":  {
                                                                                   "href":  "content/main/shared-content/configuration/text-parser/text-parser.html",
                                                                                   "title":  "Text parser",
                                                                                   "keywords":  "Text parser The adapter you are using includes the text parser component which ensures consistent parsing of text from different files. For more information on which file types are supported for your adapter, see the topics in this chapter. Designed to be a document parser, the text parser parses a semantically complete document in its entirety. The text parser produces OMF compatible output, which in turn is compatible with the OCS backing SDS (Sequential Data Store) that stores data in streams consisting of multiple values and indexes. Data types supported by the text parser The following data types are supported by the text parser: DateTime DateTimeOffset TimeSpan sbyte byte short ushort int uint long ulong float double decimal bool char string Note: Not all data types supported by the text parser are also supported by OMF. Special characters support As part of the default StreamId logic, the text parser replaces special characters as follows: Special character Replacement character * empty string \u0027 empty string ` empty string \" empty string ? empty string ; - \\| - \\ - { ( } ) [ ( ] ) Culture support Some numeric values and datetimes support cultures when they are being parsed. The default culture is en-US (US English) (InvariantCulture). OSIsoft recommends that you leave the adapter at the default unless you expect culturally variant input. Note: Installed cultures vary by machine with both Linux and Windows. If the specified culture is not installed, the text parser fails to parse input that requires that culture. Time zone support A time zone or offset specified by a time is always used to convert to UTC time. Time zones are only used if there is no offset or time zone specifier in a text date and time string. For time zones that support time changes between daylight and standard times, a text file may temporarily contain invalid or ambiguous datetimes during the time change, which are possible only for a two-hour period each year. When these time changes occur, the text parser logs them, but the datetime is parsed and passed to the callback. Ambiguous times are reported as standard times, which is the Microsoft recommendation. Date and time processing The text parser can use time zones, cultures, and custom formats to read dates and times from ingress data. You can specify date and time formats when you configure data selection. Set the date and time using the IndexFormat property. If you leave the IndexFormat property unset, the data selection configuration defaults to the ISO 8601 date format. If you are using a culture other than default en-US , use the name of day or month specific to the culture. For example, use \"Juni\" instead of \"June\" for the de-DE culture. The following date and time syntaxes have been tested and are supported. \"MM/dd/yyyy \"MM dd yyyy H:mm:ss zzz\" \"06/15/2018 \"06 15 2018 15:15:30 -05:00\" \"MM/dd/yyyy \"MM dd yyyy H:mm:ss.fff zzz\" \"06/15/2018 \"06 15 2018 15:15:30.123 -05:00\" \"dd/MM/yyyy \"dd MM yyyy H:mm:ss.fff K\" \"15/06/2018 \"15 06 2018 15:15:30.123 Z\" \"MMMM/dd/yyyy \"MMMM dd yyyy H:mm:ss.fff K\" \"June/15/2018 \"June 15 2018 15:15:30.123 Z\" (InvariantCulture/English) (InvariantCulture English) \"MMMM/dd/yyyy \"MMMM dd yyyy H:mm:ss.fff K\" \"Juni/15/2018 \"Juni 15 2018 15:15:30.123 Z\" (German) \"MMM/dd/yyyy \"MMM dd yyyy H:mm:ss.fff K\" \"Jun/15/2018 \"Jun 15 2018 15:15:30.123 Z\" \"MMM-dd-yyyy H:mm:ss.fff K\" \"Jun-15-2018 15:15:30.123 Z\" \"MMM-dd-yyyy H:mm:ss.fff K\" \"Jun-15-2018 15:15:30.123 Z\" \"MMM-dd-yyyy H:mm:ss.fff K\" \"Jun-15-2018 15:15:30.123 Z\" \"yyyy-MM-dd H:mm:ss.fff K\" \"2018-06-15 15:15:30.123 Z\" \"yyyy-M-d H:mm:ss.fff K\" \"2018-6-5 15:15:30.123 Z\" \"yyyy-M-d H:mm:ss.fff zzz\" \"2018-6-5 15:15:30.123 +05:00\" \"ddd dd MMM yyyy h:mm tt zzz\" \"Sun 15 Jun 2008 8:30 AM -06:00\" \"dddd dd MMM yyyy h:mm tt zzz\" \"Sunday 15 Jun 2008 8:30 AM -06:00\" \"dddd dd MMM yyyy h:mm tt zzz\" \"Sunday 15 Jun 2008 8:30 AM -06:00\" \"dddd dd MMMM yyyy h:mm tt zzz\" \"Sunday 15 June 2008 8:30 AM -06:00\" Adapter date and time processing uses Microsoft datetime parsing . For more documentation on standard datetime formats, which fit most use cases, see Standard date and time format strings . For documentation on custom datetime formation, see Custom date and time format strings ."
                                                                               },
    "content/main/shared-content/configuration/text-parser/xpath-and-csv-syntax-for-value-retrieval.html":  {
                                                                                                                "href":  "content/main/shared-content/configuration/text-parser/xpath-and-csv-syntax-for-value-retrieval.html",
                                                                                                                "title":  "XPath and CSV syntax for value retrieval",
                                                                                                                "keywords":  "XPath and CSV syntax for value retrieval For information on which semantics are used for retrieving values from XML and CSV files, see the following documentation: XML - XML Path Language (XPath) CSV - Column Index (1 based) or Header value (if header defined) The following syntaxes are used to extract values from XML or CSV documents. XML - Simple XPath example \u003cvalues\u003e \u003cvalue\u003e \u003ctime\u003e2020-08-10T12:10:46.0928791Z\u003c/time\u003e \u003ctime\u003e2020-08-10T12:10:46.0928791Z\u003c time\u003e \u003cvalue\u003e1.234567890\u003c/value\u003e \u003cvalue\u003e1.234567890\u003c value\u003e \u003c/value\u003e \u003c value\u003e \u003cvalue\u003e \u003ctime\u003e2020-08-10T12:10:47.0928791Z\u003c/time\u003e \u003ctime\u003e2020-08-10T12:10:47.0928791Z\u003c time\u003e \u003cvalue\u003e12.34567890\u003c/value\u003e \u003cvalue\u003e12.34567890\u003c value\u003e \u003c/value\u003e \u003c value\u003e \u003cvalue\u003e \u003ctime\u003e2020-08-10T12:10:48.0928791Z\u003c/time\u003e \u003ctime\u003e2020-08-10T12:10:48.0928791Z\u003c time\u003e \u003cvalue\u003e123.4567890\u003c/value\u003e \u003cvalue\u003e123.4567890\u003c value\u003e \u003c/value\u003e \u003c value\u003e \u003cvalue\u003e \u003ctime\u003e2020-08-10T12:10:49.0928791Z\u003c/time\u003e \u003ctime\u003e2020-08-10T12:10:49.0928791Z\u003c time\u003e \u003cvalue\u003e1234.567890\u003c/value\u003e \u003cvalue\u003e1234.567890\u003c value\u003e \u003c/value\u003e \u003c value\u003e \u003cvalue\u003e \u003ctime\u003e2020-08-10T12:10:50.0928791Z\u003c/time\u003e \u003ctime\u003e2020-08-10T12:10:50.0928791Z\u003c time\u003e \u003cvalue\u003e12345.67890\u003c/value\u003e \u003cvalue\u003e12345.67890\u003c value\u003e \u003c/value\u003e \u003c value\u003e \u003cvalue\u003e \u003ctime\u003e2020-08-10T12:10:51.0928791Z\u003c/time\u003e \u003ctime\u003e2020-08-10T12:10:51.0928791Z\u003c time\u003e \u003cvalue\u003e123456.7890\u003c/value\u003e \u003cvalue\u003e123456.7890\u003c value\u003e \u003c/value\u003e \u003c value\u003e \u003cvalue\u003e \u003ctime\u003e2020-08-10T12:10:52.0928791Z\u003c/time\u003e \u003ctime\u003e2020-08-10T12:10:52.0928791Z\u003c time\u003e \u003cvalue\u003e12345678.90\u003c/value\u003e \u003cvalue\u003e12345678.90\u003c value\u003e \u003c/value\u003e \u003c value\u003e \u003cvalue\u003e \u003ctime\u003e2020-08-10T12:10:53.0928791Z\u003c/time\u003e \u003ctime\u003e2020-08-10T12:10:53.0928791Z\u003c time\u003e \u003cvalue\u003e123456789.0\u003c/value\u003e \u003cvalue\u003e123456789.0\u003c value\u003e \u003c/value\u003e \u003c value\u003e \u003c/values\u003e \u003c values\u003e The following XPath configuration reads a series of values: { \"Id\": \"DoubleValue\", \"FieldDefinition\": \"./values/value/value\", \". values value value\", \"DataType\": \"Double\" }, { \"Id\": \"Timestamp\", \"FieldDefinition\": \"./values/value/time\", \". values value time\", \"DataType\": \"DateTime\", \"IsIndex\": true } CSV - Simple CSV column index example 2020-08-10T12:10:46.0928791Z,1.234567890 2020-08-10T12:10:47.0928791Z,12.34567890 2020-08-10T12:10:48.0928791Z,123.4567890 2020-08-10T12:10:49.0928791Z,1234.567890 2020-08-10T12:10:50.0928791Z,12345.67890 2020-08-10T12:10:51.0928791Z,123456.7890 2020-08-10T12:10:52.0928791Z,12345678.90 2020-08-10T12:10:53.0928791Z,123456789.0 The following CSV column index configuration requires the text parser be configured with HasHeader=false . The column indexes are 1 based and configured as strings. { \"Id\": \"DoubleValue\", \"FieldDefinition\": \"2\", \"DataType\": \"Double\" }, { \"Id\": \"Timestamp\", \"FieldDefinition\": \"1\", \"DataType\": \"DateTime\", \"IsIndex\": true } CSV - Simple CSV column header example Date,Value 2020-08-10T12:10:46.0928791Z,1.234567890 2020-08-10T12:10:47.0928791Z,12.34567890 2020-08-10T12:10:48.0928791Z,123.4567890 2020-08-10T12:10:49.0928791Z,1234.567890 2020-08-10T12:10:50.0928791Z,12345.67890 2020-08-10T12:10:51.0928791Z,123456.7890 2020-08-10T12:10:52.0928791Z,12345678.90 2020-08-10T12:10:53.0928791Z,123456789.0 The following CSV column header configuration requires the text parser be configured with HasHeader=true . { \"Id\": \"DoubleValue\", \"FieldDefinition\": \"Value\", \"DataType\": \"Double\" }, { \"Id\": \"Timestamp\", \"FieldDefinition\": \"Date\", \"DataType\": \"DateTime\", \"IsIndex\": true }"
                                                                                                            },
    "content/main/shared-content/diagnostics/diagnostics.html":  {
                                                                     "href":  "content/main/shared-content/diagnostics/diagnostics.html",
                                                                     "title":  "Diagnostics",
                                                                     "keywords":  "Diagnostics The adapter and its components produce various kinds of diagnostics data that is sent to all health endpoints. The System_Diagnostics.json file contains a flag that determines whether diagnostics are enabled. You can change this at runtime through REST calls or the EdgeCmd utility. Diagnostics data are collected by default. To egress diagnostics related data, you have to configure an adapter health endpoint first. See Health endpoint configuration . Available diagnostics data Every minute, dynamic data is sent to configured health endpoints. The following diagnostics data are available: System Stream count IO rate Error rate"
                                                                 },
    "content/main/shared-content/diagnostics/egress.html":  {
                                                                "href":  "content/main/shared-content/diagnostics/egress.html",
                                                                "title":  "Egress",
                                                                "keywords":  "Egress The Egress component of the adapter produces the following diagnostics stream: IO rate The Diagnostics.Egress.IORate dynamic type includes the following values, which are logged in a stream with the Id {machineName}.{serviceName}.OmfEgress.{EndpointId}.IORate . IORate includes only sequential data successfully sent to an egress endpoint. Property Type Description timestamp string Timestamp of event IORate double One-minute rolling average of data rate (streams/second) (streams second)"
                                                            },
    "content/main/shared-content/diagnostics/error-rate.html":  {
                                                                    "href":  "content/main/shared-content/diagnostics/error-rate.html",
                                                                    "title":  "Error rate",
                                                                    "keywords":  "Error rate The Diagnostics.Adapter.ErrorRate dynamic type includes the following values, which are logged in a stream with the Id {componentid}.ErrorRate . Property Type Description timestamp string Timestamp of event ErrorRate double One-minute rolling average of error rate (streams/second) (streams second)"
                                                                },
    "content/main/shared-content/diagnostics/io-rate.html":  {
                                                                 "href":  "content/main/shared-content/diagnostics/io-rate.html",
                                                                 "title":  "IO rate",
                                                                 "keywords":  "IO rate The Diagnostics.Adapter.IORate dynamic type includes the following values, which are logged in a stream with the Id {componentid}.IORate . IORate includes only sequential data collected from a data source. Property Type Description timestamp string Timestamp of event IORate double One-minute rolling average of data rate (streams/second) (streams second)"
                                                             },
    "content/main/shared-content/diagnostics/stream-count.html":  {
                                                                      "href":  "content/main/shared-content/diagnostics/stream-count.html",
                                                                      "title":  "Stream count",
                                                                      "keywords":  "Stream count The Diagnostics.StreamCountEvent dynamic type includes the following values, which are logged in a stream with the Id {componentid}.StreamCount . The StreamCount and TypeCount include only types and streams created for sequential data received from a data source. Property Type Description timestamp string Timestamp of event StreamCount int Number of streams created by the adapter instance TypeCount int Number of types created by the adapter instance"
                                                                  },
    "content/main/shared-content/diagnostics/system.html":  {
                                                                "href":  "content/main/shared-content/diagnostics/system.html",
                                                                "title":  "System",
                                                                "keywords":  "System The Diagnostics.System dynamic type includes the following values which are logged in a stream with the Id System.Diagnostics . This diagnostic stream contains system level information related to the host platform that the adapter is running on. Property Type Description timestamp string Timestamp of event ProcessIdentifier int Process Id of the host process StartTime string Time at which the host process started WorkingSet long Amount of physical memory in bytes, allocated for the host process TotalProcessorTime double Total processor time for the host process expressed in seconds TotalUserProcessorTime double User processor time for the host process expressed in seconds TotalPrivilegedProcessorTime double Privileged processor time for the host process expressed in seconds ThreadCount int Number of threads in the host process HandleCount int Number of handles opened by the host process ManagedMemorySize double Number of bytes currently thought to be allocated in managed memory Unit of Measure = megabytes PrivateMemorySize double Amount of paged memory in bytes allocated for the host process Unit of Measure = megabytes PeakPagedMemorySize double Maximum amount of memory in the virtual memory paging file in bytes used by the host process. Unit of Measure = megabytes StorageTotalSize double Total size of the storage medium in use by the system Unit of Measure = megabytes StorageFreeSpace double Free space available Unit of Measure = megabytes Each adapter component produces its own diagnostics streams."
                                                            },
    "content/main/shared-content/health/device-status.html":  {
                                                                  "href":  "content/main/shared-content/health/device-status.html",
                                                                  "title":  "Device status",
                                                                  "keywords":  "Device status The device status indicates the health of this component and if it is currently communicating properly with the data source. This time-series data is stored within a PI point or OCS stream, depending on the endpoint type. During healthy steady-state operation, a value of Good is expected. Property Type Description Time string Timestamp of the event DeviceStatus string The value of the DeviceStatus The possible statuses are: Status Meaning Good The component is connected to the data source and it is collecting data. ConnectedNoData The component is connected to the data source but it is not receiving data from it. Starting The component is currently in the process of starting up and is not yet connected to the data source. DeviceInError The component encountered an error either while connecting to the data source or attempting to collect data. Shutdown The component is either in the process of shutting down or has finished. Removed The adapter component has been removed and will no longer collect data. NotConfigured The adapter component has been created but is not yet configured."
                                                              },
    "content/main/shared-content/health/health.html":  {
                                                           "href":  "content/main/shared-content/health/health.html",
                                                           "title":  "Health",
                                                           "keywords":  "Health PI Adapters produce various kinds of health data that can be egressed to different health endpoints. To egress health related data, you have to configure an adapter health endpoint first. See Health endpoint configuration . Available health data Dynamic data is sent every minute to configured health endpoints. The following health data is available: Device status Next Health Message Expected"
                                                       },
    "content/main/shared-content/health/health-and-diagnostics.html":  {
                                                                           "href":  "content/main/shared-content/health/health-and-diagnostics.html",
                                                                           "title":  "Health and Diagnostics",
                                                                           "keywords":  "Health and Diagnostics PI Adapters produce various types of health data. You can use health data to ensure that your adapters are running properly and that data flows to the configured OMF endpoints. For more information on available adapter health data, see health . PI Adapters also produce diagnostic data. You can use diagnostic data to find more information about a particular adapter instance. Diagnostic data lives alongside the health data and you can egress it using a health endpoint and setting EnableDiagnostics to true . You can configure EnableDiagnostics in the system\u0027s General configuration . For more information on available adapter diagnostics data, see diagnostics . In OSIsoft Cloud Services (OCS), both health and diagnostics data are created as assets. The data are available in the Asset Explorer and you can use them in the OCS Trend feature. For more information, see the OCS documentation Assets . Health endpoint differences Two OMF endpoints are currently supported for adapter health data: PI Web API OSIsoft Cloud Services (OCS) There are a few differences in how these two systems treat the associated health and diagnostics data. PI Web API parses the information and sends it to configured PI servers for the OMF endpoint. The static data is used to create an AF structure on a PI AF server. The dynamic health data is time-series data that is stored in PI points on a PI Data Archive. You can see it in the AF structure as PI point data reference attributes. OCS does not currently provide a way to store the static metadata. For OCS-based adapter health endpoints, only the dynamic data is stored. Each value is its own stream with the timestamp property as the single index. AF structure With a health endpoint configured to a PI server, you can use PI System Explorer to view the health and diagnostics of an adapter. The element hierarchy is shown in the following image. The Elements root contains a link to an Adapters node. This is the root node for all adapter instances. Below Adapters , you will find one or more adapter nodes. Each node\u0027s title is defined by the node\u0027s corresponding computer name and service name in this format: {ComputerName}.{ServiceName} . For example, in the following image, MachineName is the computer name and OpcUa is the service name. To see the health and diagnostics values, click on an adapter node and select Attributes ."
                                                                       },
    "content/main/shared-content/health/next-health-message-expected.html":  {
                                                                                 "href":  "content/main/shared-content/health/next-health-message-expected.html",
                                                                                 "title":  "Next health message expected",
                                                                                 "keywords":  "Next health message expected This property is similar to a heartbeat. A new value for NextHealthMessageExpected is sent by an individual adapter data component on a periodic basis while it is functioning properly. This value is a timestamp that indicates when the next value should be received. When monitoring, if the next value is not received by the indicated time, this likely means that there is an issue. It could be an issue with the adapter, adapter component, network connection between the health endpoint and the adapter, and so on. Property Type Description Time string Timestamp of the event NextHealthMessageExpected string Timestamp when next value is expected"
                                                                             },
    "content/main/shared-content/installation/installation.html":  {
                                                                       "href":  "content/main/shared-content/installation/installation.html",
                                                                       "title":  "Installation",
                                                                       "keywords":  "Installation Adapters are installed on a local machine using an install kit downloaded from the OSIsoft Customer Portal. For instructions on downloading and installing adapters, see Install the adapter . Alternatively, you can build custom installers or containers for Linux. For more information, see the Docker instructions in the documentation of the respective adapter."
                                                                   },
    "content/main/shared-content/installation/install-the-adapter.html":  {
                                                                              "href":  "content/main/shared-content/installation/install-the-adapter.html",
                                                                              "title":  "Install the adapter",
                                                                              "keywords":  "Install the adapter You can install adapters on either a Windows or Linux operating system. Before installing the adapter, see the respective system requirements to ensure your machine is properly configured to provide optimum adapter operation. Windows Complete the following steps to install a PI adapter on a Windows computer: Download PI-Adapter-for-Azure-Event-Hubs-1.0.1.239-x64_.msi from the OSIsoft Customer portal . Note: Customer login credentials are required to access the portal. Run PI-Adapter-for-Azure-Event-Hubs-1.0.1.239-x64_.msi file. Follow the setup wizard. You can change the installation folder or port number during setup. The default port number is 5590 . Optional: To verify the installation, run the following curl command with the port number that you specified during installation: curl http://localhost:5590/api/v1/configuration http:  localhost:5590 api v1 configuration If you receive an error, wait a few seconds and try the script again. If the installation was successful, a JSON copy of the default system configuration is returned. Linux Complete the following steps to install an adapter on a Linux computer: Download the appropriate Linux distribution file ( PI-Adapter-for-Azure-Event-Hubs-1.0.1.239- platform _.deb ) from the OSIsoft Customer portal . Note: Customer login credentials are required to access the portal. Open a terminal. Run the sudo apt update command to update available packages information. Run the sudo apt install command against the Linux distribution file ( PI-Adapter-for-Azure-Event-Hubs-1.0.1.239- platform _.deb ) selected in step 1 of this procedure. For example: sudo apt install ./PI-Adapter-for-Azure-Event-Hubs-1.0.1.239-x64_.deb . PI-Adapter-for-Azure-Event-Hubs-1.0.1.239-x64_.deb Optional: To verify the installation, run the following curl command with the port number that you specified during installation: curl http://localhost:5590/api/v1/configuration http:  localhost:5590 api v1 configuration If you receive an error, wait a few seconds and run the command again. If the installation was successful, a JSON copy of the default system configuration is returned."
                                                                          },
    "content/main/shared-content/installation/install-using-docker.html":  {
                                                                               "href":  "content/main/shared-content/installation/install-using-docker.html",
                                                                               "title":  "Installation using Docker",
                                                                               "keywords":  "Installation using Docker Docker is a set of tools that you can use on Linux to manage application deployments. This topic provides examples of how to create a Docker container with the adapter. Note: The use of Docker is only recommended if your environment requires it. Only users proficient with Docker should use it to install the adapter. Docker is not required to use the adapter. Create a startup script To create a startup script for the adapter, follow the instructions below. Use a text editor to create a script similar to one of the following examples: Note: The script varies slightly by processor. ARM32 #!/bin/sh #! bin sh if [ -z $portnum ] ; then exec /PI-Adapter-for-Azure-Event-Hubs_1.0.1.239-arm_/OSIsoft.Data.System.Host  PI-Adapter-for-Azure-Event-Hubs_1.0.1.239-arm_ OSIsoft.Data.System.Host else exec /PI-Adapter-for-Azure-Event-Hubs_1.0.1.239-arm_/OSIsoft.Data.System.Host  PI-Adapter-for-Azure-Event-Hubs_1.0.1.239-arm_ OSIsoft.Data.System.Host --port:$portnum fi ARM64 #!/bin/sh #! bin sh if [ -z $portnum ] ; then exec /PI-Adapter-for-Azure-Event-Hubs_1.0.1.239-arm64_/OSIsoft.Data.System.Host  PI-Adapter-for-Azure-Event-Hubs_1.0.1.239-arm64_ OSIsoft.Data.System.Host else exec /PI-Adapter-for-Azure-Event-Hubs_1.0.1.239-arm64_/OSIsoft.Data.System.Host  PI-Adapter-for-Azure-Event-Hubs_1.0.1.239-arm64_ OSIsoft.Data.System.Host --port:$portnum fi AMD64 #!/bin/sh #! bin sh if [ -z $portnum ] ; then exec /PI-Adapter-for-Azure-Event-Hubs_1.0.1.239-x64_/OSIsoft.Data.System.Host  PI-Adapter-for-Azure-Event-Hubs_1.0.1.239-x64_ OSIsoft.Data.System.Host else exec /PI-Adapter-for-Azure-Event-Hubs_1.0.1.239-x64_/OSIsoft.Data.System.Host  PI-Adapter-for-Azure-Event-Hubs_1.0.1.239-x64_ OSIsoft.Data.System.Host --port:$portnum fi Name the script eventhubsdockerstart.sh and save it to the directory where you plan to create the container. Create a Docker container To create a Docker container that runs the adapter, follow the instructions below. Create the following Dockerfile in the directory where you want to create and run the container. Note: Dockerfile is the required name of the file. Use the variation according to your operating system: ARM32 FROM ubuntu WORKDIR /   RUN apt-get update \u0026\u0026 DEBIAN_FRONTEND=noninteractive apt-get install -y ca-certificates libicu60 libssl1.1 curl COPY eventhubsdockerstart.sh /   RUN chmod +x /eventhubsdockerstart.sh  eventhubsdockerstart.sh ADD ./PI-Adapter-for-Azure-Event-Hubs_1.0.1.239-arm_.tar.gz . PI-Adapter-for-Azure-Event-Hubs_1.0.1.239-arm_.tar.gz . ENTRYPOINT [\"/eventhubsdockerstart.sh\"] [\" eventhubsdockerstart.sh\"] ARM64 FROM ubuntu WORKDIR /   RUN apt-get update \u0026\u0026 DEBIAN_FRONTEND=noninteractive apt-get install -y ca-certificates libicu66 libssl1.1 curl COPY eventhubsdockerstart.sh /   RUN chmod +x /eventhubsdockerstart.sh  eventhubsdockerstart.sh ADD ./PI-Adapter-for-Azure-Event-Hubs_1.0.1.239-arm64_.tar.gz . PI-Adapter-for-Azure-Event-Hubs_1.0.1.239-arm64_.tar.gz . ENTRYPOINT [\"/eventhubsdockerstart.sh\"] [\" eventhubsdockerstart.sh\"] AMD64 (x64) FROM ubuntu WORKDIR /   RUN apt-get update \u0026\u0026 DEBIAN_FRONTEND=noninteractive apt-get install -y ca-certificates libicu66 libssl1.1 curl COPY eventhubsdockerstart.sh /   RUN chmod +x /eventhubsdockerstart.sh  eventhubsdockerstart.sh ADD ./PI-Adapter-for-Azure-Event-Hubs_1.0.1.239-x64_.tar.gz . PI-Adapter-for-Azure-Event-Hubs_1.0.1.239-x64_.tar.gz . ENTRYPOINT [\"/eventhubsdockerstart.sh\"] [\" eventhubsdockerstart.sh\"] Copy the PI-Adapter-for-Azure-Event-Hubs-1.0.1.239- platform _.tar.gz file to the same directory as the Dockerfile . Copy the eventhubsdockerstart.sh script to the same directory as the Dockerfile . Run the following command line in the same directory ( sudo may be necessary): docker build -t eventhubsadapter . Docker container startup The following procedures contain instructions on how to run the adapter inside a Docker container with different options enabled. Run the Docker container with REST access enabled To run the adapter inside a Docker container with access to its REST API from the local host, complete the following steps: Use the docker container image eventhubsadapter created previously. Type the following in the command line ( sudo may be necessary): docker run -d --network host eventhubsadapter Port 5590 is accessible from the host and you can make REST calls to the adapter from applications on the local host computer. In this example, all data stored by the adapter is stored in the container itself. When you delete the container, the stored data is also deleted. Run the Docker container with persistent storage To run the adapter inside a Docker container while using the host for persistent storage, complete the following steps. This procedure also enables access to the adapter REST API from the local host. Use the docker container image eventhubsadapter created previously. Type the following in the command line ( sudo may be necessary): docker run -d --network host -v /eventhubs:/usr/share/OSIsoft/  eventhubs: usr share OSIsoft  eventhubsadapter Port 5590 is accessible from the host and you can make REST calls to the adapter from applications on the local host computer. In this example, all data that is written to the container is instead written to the host directory and the host directory is a directory on the local machine, /eventhubs  eventhubs . You can specify any directory. Change port number To use a different port other than 5590 , you can specify a portnum variable on the docker run command line. For example, to start the adapter using port 6000 instead of 5590 , use the following command: docker run -d -e portnum=6000 --network host eventhubsadapter This command accesses the REST API with port 6000 instead of port 5590 . The following curl command returns the configuration for the container. curl http://localhost:6000/api/v1/configuration http:  localhost:6000 api v1 configuration Remove REST access If you remove the --network host option from the docker run command, REST access is not possible from outside the container. This may be of value where you want to host an application in the same container as the adapter but do not want to have external REST access enabled."
                                                                           },
    "content/main/shared-content/installation/system-requirements.html":  {
                                                                              "href":  "content/main/shared-content/installation/system-requirements.html",
                                                                              "title":  "System requirements",
                                                                              "keywords":  "System requirements PI Adapter for Azure Event Hubs is supported on a variety of platforms and processors. Install kits are available for the following platforms: Operating System Platform Installation Kit Processor(s) Windows 10 Enterprise Windows 10 IoT Enterprise x64 PI-Adapter-for-Azure-Event-Hubs-1.0.1.239-x64_.msi Intel/AMD Intel AMD 64-bit processors Debian 9, 10 Ubuntu 18.04, 20.04 x64 PI-Adapter-for-Azure-Event-Hubs-1.0.1.239-x64_.deb Intel/AMD Intel AMD 64-bit processors Debian 9, 10 Ubuntu 20.04 ARM32 PI-Adapter-for-Azure-Event-Hubs-1.0.1.239-arm_.deb ARM 32-bit processors Debian 10 Ubuntu 18.04, 20.04 ARM64 PI-Adapter-for-Azure-Event-Hubs-1.0.1.239-arm64_.deb ARM 64-bit processors Alternatively, you can use tar.gz files with binaries to build your own custom installers or containers for Linux. For more information on installing the adapter with Docker containers, see Installation using Docker . PI Web API compatibility This version of PI Adapter for Azure Event Hubs is compatible with PI Web API 2021 and later."
                                                                          },
    "content/main/shared-content/installation/uninstall-the-adapter.html":  {
                                                                                "href":  "content/main/shared-content/installation/uninstall-the-adapter.html",
                                                                                "title":  "Uninstall the adapter",
                                                                                "keywords":  "Uninstall the adapter Complete the procedure corresponding to your specific operating system to uninstall the adapter: Windows To delete the PI adapter program files from a Windows device, use the Windows Control Panel uninstall application process. Note: The configuration, data, and log files are not deleted by the uninstall process. Optional: To delete data, configuration, and log files, delete the directory: %ProgramData%\\OSIsoft\\Adapters\\EventHubs This deletes all data processed by the adapter, in addition to the configuration and log files. Linux To delete PI Adapter software from a Linux device, open a terminal window and run the following command: sudo apt remove pi.adapter.EventHubs Optional: To delete data, configuration, and log files, run the following command: sudo rm -r /usr/share/OSIsoft/Adapters/EventHubs  usr share OSIsoft Adapters EventHubs This deletes all data processed by the adapter, in addition to the configuration and log files."
                                                                            },
    "content/main/shared-content/installation/upgrade-the-adapter.html":  {
                                                                              "href":  "content/main/shared-content/installation/upgrade-the-adapter.html",
                                                                              "title":  "Upgrade the adapter",
                                                                              "keywords":  "Upgrade the adapter When a new version of the adapter is released, you can upgrade to the latest version by running the new installation package. Windows upgrade Complete the following steps to upgrade a PI adapter on a Windows computer: Download PI-Adapter-for-Azure-Event-Hubs-1.0.1.239-x64_.msi from the OSIsoft Customer Portal . Note: Customer login credentials are required to access the portal. Run PI-Adapter-for-Azure-Event-Hubs-1.0.1.239-x64_.msi . Complete the setup wizard. Optional: To verify the upgrade, run the following curl command with the port number that you specified when completing the wizard: curl -X GET \"http://localhost:5590/api/v1/Diagnostics/ProductInformation\" \"http:  localhost:5590 api v1 Diagnostics ProductInformation\" Upon successful upgrade, the JSON response lists the updated application version: { \"Application Version\": \"1.0.1.239\", //    upgraded version \".Net Core Version\": \".NET Core 3.1.15\", \"Operating System\": \"Microsoft Windows 10.0.19041\" } Linux upgrade Complete the following steps to upgrade a PI adapter on a Linux computer: Download the appropriate Linux distribution file from the OSIsoft Customer Portal . Note: Customer login credentials are required to access the portal. Open a terminal session. Move the Linux distribution file to the target host and run the sudo apt upgrade command. Platform Command Linux x64 sudo apt upgrade ./PI-Adapter-for-Azure-Event-Hubs-1.0.1.239-x64_.deb . PI-Adapter-for-Azure-Event-Hubs-1.0.1.239-x64_.deb Linux ARM32 Debian sudo apt upgrade ./PI-Adapter-for-Azure-Event-Hubs-1.0.1.239-arm_.deb . PI-Adapter-for-Azure-Event-Hubs-1.0.1.239-arm_.deb Linux ARM64 Debian sudo apt upgrade ./PI-Adapter-for-Azure-Event-Hubs-1.0.1.239-arm64_.deb . PI-Adapter-for-Azure-Event-Hubs-1.0.1.239-arm64_.deb Optional: To verify the upgrade, run the following curl command with the port number that you specified: curl -X GET \"http://localhost:5590/api/v1/Diagnostics/ProductInformation\" \"http:  localhost:5590 api v1 Diagnostics ProductInformation\" Upon successful upgrade, the JSON response lists the updated application version: { \"Application Version\": \"1.0.1.239\", //    upgraded version \".Net Core Version\": \".NET Core 3.1.15\", \"Operating System\": \"Microsoft Windows 10.0.19041\" }"
                                                                          },
    "content/main/shared-content/introduction/intro-to-pi-adapters.html":  {
                                                                               "href":  "content/main/shared-content/introduction/intro-to-pi-adapters.html",
                                                                               "title":  "PI Adapter for Azure Event Hubs 1.0.1.239",
                                                                               "keywords":  "PI Adapter for Azure Event Hubs 1.0.1.239 PI Adapter for Azure Event Hubs is a data collection technology that collects time-series operations data from a data source over the protocol and then sends it to a supported storage location in the Open Message Format (OMF). \u003c!-- Add content about the protocol here --\u003e PI Adapter for Azure Event Hubs data flow The following diagram depicts the collection and processing of data for an operational PI Adapter for Azure Event Hubs, collecting and processing data. Refer to the list below the diagram for more information on each callout depicted. \u003c!-- Mark Bishop 3/3/22: 3 3 22: The SVG file referenced below can be opened and edited using https://app.diagrams.net/ https:  app.diagrams.net  --\u003e The user installs and configures PI Adapter for Azure Event Hubs on a host system. You can configure the adapter using either a REST interface or EdgeCmd, a command line utility specifically designed for interfacing with edge systems. The adapter collects data from assets over the protocol, a process known as data ingress . The adapter converts ingress data to the Open Message Format (OMF), a format that supported storage locations understand. The adapter sends OMF data to a supported storage location in a process known as data egress . Supported egress endpoints include: PI Server OSIsoft Cloud Services"
                                                                           },
    "content/main/shared-content/metadata/metadata.html":  {
                                                               "href":  "content/main/shared-content/metadata/metadata.html",
                                                               "title":  "Metadata",
                                                               "keywords":  "Metadata If the metadataLevel is set to Low , Medium , or High in the General configuration , adapter streams created by the ingress components include the following metadata: Datasource: {ComponentId} AdapterType: {ComponentType} ComponentId corresponds to the adapter components\u0027 data source configured in the Components configuration . ComponentType corresponds to the adapter type. Metadata for health and diagnostics streams If you configure a health endpoint and enable metadata, they are included in the health streams ( Device status and Next health message expected ) together with ComponentId and ComponentType . If you enable diagnostics in General configuration , metadata are included in the diagnostics streams ( Stream count , IO rate , Error rate ) together with ComponentId and ComponentType . The adapter may also send its own stream metadata not including health and diagnostics streams. For more information about what custom metadata is included in each stream, see the user guide for your adapter. Note: Metadata is only sent for streams created by the ingress components. Currently, the only endpoint that persists sent metadata is OCS (OSIsoft Cloud Services)."
                                                           },
    "content/main/shared-content/technical-support-and-feedback.html":  {
                                                                            "href":  "content/main/shared-content/technical-support-and-feedback.html",
                                                                            "title":  "Technical support and feedback",
                                                                            "keywords":  "Technical support and feedback OSIsoft provides several ways to report issues and provide feedback on PI Adapters. Technical support For technical assistance with PI Adapters, contact OSIsoft Technical Support through the OSIsoft Customer Portal . We can help you identify the problem, provide workarounds and address any concerns you may have. Remote access to your facilities may be necessary during the session. Note: You must have an account set up in the OSIsoft Customer Portal before you can open a case. If you do not have a portal account, see How to Get a Login to OSIsoft Customer Portal . Alternatively, call OSIsoft Technical Support at +1 510-297-5828. When you contact OSIsoft Technical Support, be prepared to provide this information: Product name, version, and build numbers Details about your computer platform (CPU type, operating system, and version number) Time that the difficulty started Log files at that time Details of any environment changes prior to the start of the issue Summary of the issue, including any relevant log files during the time the issue occurred \u003c!--To view a brief primer on PI Adapters, see the [PI Adapters playbook](https://customers.osisoft.com/s/knowledgearticle?knowledgeArticleUrl=Playbook-PI-adapters) playbook](https:  customers.osisoft.com s knowledgearticle?knowledgeArticleUrl=Playbook-PI-adapters) in the OSIsoft Customer Portal.--\u003e Product feedback To submit product feedback for PI Adapters, visit the PI Adapters feedback page . The product team at OSIsoft regularly monitors the page. Documentation feedback To submit documentation feedback for PI Adapters, send an email to documentation@aveva.com . Be sure to include the following information with your feedback: Product name and version Documentation topic URL Details of the suggestion or error The technical documentation team will review and address your feedback in future documentation updates."
                                                                        },
    "content/main/shared-content/troubleshooting/troubleshooting.html":  {
                                                                             "href":  "content/main/shared-content/troubleshooting/troubleshooting.html",
                                                                             "title":  "Troubleshooting",
                                                                             "keywords":  "Troubleshooting PI adapters provide features for troubleshooting issues related to connectivity, data flow, and configuration. Resources include adapter logs and the Wireshark troubleshooting tool . If you are still unable to resolve issues or need additional guidance, contact OSIsoft Technical Support through the OSIsoft Customer Portal . Note: Make sure to also check the troubleshooting information specific to your adapter in this user guide. Logs Messages from the System and OmfEgress logs provide information on the status of the adapter. For example, they show if a connection from the adapter to an egress endpoint exists. Perform the following steps to view the System and OmfEgress logs: Navigate to the logs directory: Windows: %ProgramData%\\OSIsoft\\Adapters\\\u003cAdapterName\u003e\\Logs Linux: /usr/share/OSIsoft/Adapters/\u003cAdapterName\u003e/Logs  usr share OSIsoft Adapters \u003cAdapterName\u003e Logs . Example: A successful connection to a PI Web API egress endpoint displays the following message in the OmfEgress log: 2020-11-02 11:08:51.870 -06:00 [Information] Data will be sent to the following OMF endpoint: Id: \u003comfegress id\u003e Endpoint: \u003cpi web api URL\u003e (note: the pi web api default port is 443) ValidateEndpointCertificate: \u003ctrue or false\u003e Optional: Change the log level of the adapter to receive more information and context. For more information, see Logging configuration . ASP .NET Core platform log The ASP .NET Core platform log provides information from the Kestrel web server that hosts the application. The log could contain information that the adapter is overloaded with incoming data. Perform the following steps to spread the load among multiple adapters: Decrease the scan frequency. Lower the amount of data selection items. Wireshark Wireshark is a protocol-specific troubleshooting tool that supports all current adapter protocols. Perform the following steps if you want to use Wireshark to capture traffic from the data source to the adapter or from the adapter to the OMF destination. Download Wireshark . Familiarize yourself with the tool and read the Wireshark user guide . Health and diagnostics egress to PI Web API The adapter sends health and diagnostics data to PI Web API; in some cases, conflicts may occur that are due to changes or perceived changes in PI Web API. For example, a 409 - Conflict error message displays if you upgrade your adapter version and the PI points do not match in the upgraded version. However, data is continued to be sent as long as containers are created, so buffering only starts if no containers are created. To resolve the conflict, perform the following steps: Stop the adapter. Delete the Health folder inside of the Buffers folder. Stop PI Web API. Delete the relevant adapter created AF structure. Delete the associated health and diagnostics PI points on any or all PI Data Archives created by PI Web API. Start PI Web API. Start the adapter. Adapter connection to egress endpoint Certain egress health information in both PI Web API and OCS show if an adapter connection to an egress endpoint exists. To verify an active connection, perform one of the following procedures: PI Web API connection Perform the following steps to determine if a connection to the PI Web API endpoint exists: Open PI Web API. Select the OmfEgress component of your adapter, for example GVAdapterUbuntu.\u003cAdapterName\u003e.OmfEgress . Make sure that the following PI points have been created for your egress endpoint: DeviceStatus NextHealthMessageExpected IORate OCS connection Perform the following steps to determine if a connection to the OCS endpoint exists: Open OCS. Click Sequential Data Store \u003e Streams . Makes sure that the following streams have been created for your egress endpoint: DeviceStatus NextHealthMessageExpected IORate TCP connection Perform the following steps to see all established TCP sessions in Linux: Open a terminal. Type the following command: ss -o state established -t -p Press Enter."
                                                                         },
    "content/pi-adapter-for-azure-event-hubs-overview/pi-adapter-for-azure-event-hubs-performance-best-practices.html":  {
                                                                                                                             "href":  "content/pi-adapter-for-azure-event-hubs-overview/pi-adapter-for-azure-event-hubs-performance-best-practices.html",
                                                                                                                             "title":  "Performance best practices",
                                                                                                                             "keywords":  "Performance best practices The performance of an adapter, PI Adapter for Azure Event Hubs, specifically, depends upon a number of factors, like egress limits, data shape, and data selections. Use the following performance guidelines and best practices when you set up an adapter. Note: A many-to-one relationship exists between JSON messages sent to Event Hub and time indexed events sent to the data endpoints. For more information, see Throughput units . Data shape PI Adapter for Azure Event Hubs assumes a single schema per source Event Hub. When a data selection is assigned a particular Event Hub, it is assumed that all messages received from that Event Hub should be processed for the IndexField and ValueField . OSIsoft recommends using larger messages rather than smaller messages for the same amount of data to achieve optimal performance. Example: 5000 stream values per message at a 250 ms update rate perform better than 100 streams per message at a 5 ms update rate. OSIsoft also recommends that you use flat data rather than deeply nested data as they are not as expensive to parse. More complex messages may also require more expensive JSONPath expressions to identify values and timestamps, which results in lower performance. Flat data { \"Timestamp\": \"2020-01-01T00:00:00Z\", \"Events\": [ { \"Id\": \"1\", \"Value\": 0 }, { \"Id\": \"2\", \"Value\": 0 } ] } Nested data { \"Timestamp\": \"2020-01-01T00:00:00Z\", \"L1\": { \"L2\": { \"L3\": { \"L4\": [ { \"Id\": \"1\", \"Value\": 0 }, { \"Id\": \"2\", \"Value\": 0 } ] } } } } Data selections Data selection related to the Event Hubs and the configuration of the ValueField and IndexField parameters highly affect performance of the adapter. Event Hub Region An Event Hub namespace in the same geographic region as the adapter ensures minimal latency due to network communications. Partitions The OCS SDS service and PI Data Archive can both store sequential data. Out of order data is accepted; however, OSIsoft recommends to order your data for optimal performance. OSIsoft further recommends that you use a single partition to preserve message order even though Azure Event Hubs support multiple partitions for a single Event Hub for parallelism downstream. Throughput units A single throughput unit for Azure Event Hubs allows for ingress of up to 1MB per second or 1000 events (JSON messages). Each message contains as many individual data endpoint events as are configured in the data selections for that Event Hub. Example: 100 data selections that use Event Hub A and 10 events that are sent to the Event Hub every second results in 1000 events per second sent to the data endpoint. Consider this many-to-one relationship between the events sent to Event Hub and events sent to the data endpoint when you select the appropriate throughput units for your Event Hub. ValueField and IndexField JSONPath OSIsoft recommends that you use the most direct reference possible to refer to a data item when you configure data selections. Evaluating JSONPath expressions can be expensive. Sample data { \"Timestamp\": \"2020-01-01T00:00:00Z\", \"Events\": [ { \"Id\": \"1\", \"Value\": 0 }, { \"Id\": \"2\", \"Value\": 0 } ] } The following data selections retrieve the same value; however, note the differences in both ValueField values. Poor performance This example has a query $.Events[?(@.Id == \"1\")].Value of the events collection. { \"Selected\": true, \"Name\": null, \"StreamId\": \"EventHubs_1\", \"DataFilterId\": null, \"EventHubName\": \"hub\", \"DeviceId\": null, \"ValueField\": \"$.Events[?(@.Id == \\u00271\\u0027)].Value\", \"IndexField\": \"$.TimeStamp\", \"IndexFormat\": null, \"DataType\": \"string\" } High performance This example uses a direct reference $.Events[1].Value to the index. { \"Selected\": true, \"Name\": null, \"StreamId\": \"EventHubs_1\", \"DataFilterId\": null, \"EventHubName\": \"hub\", \"DeviceId\": null, \"ValueField\": \"$.Events[1].Value\", \"IndexField\": \"$.TimeStamp\", \"IndexFormat\": null, \"DataType\": \"string\" }"
                                                                                                                         },
    "content/pi-adapter-for-azure-event-hubs-overview/pi-adapter-for-azure-event-hubs-principles-of-operation.html":  {
                                                                                                                          "href":  "content/pi-adapter-for-azure-event-hubs-overview/pi-adapter-for-azure-event-hubs-principles-of-operation.html",
                                                                                                                          "title":  "Principles of operation",
                                                                                                                          "keywords":  "Principles of operation This adapter\u0027s operations focus on data collection and stream creation. Adapter configuration For the Azure Event Hubs adapter to start data collection, configure the following items: Data source: Provide the data source from which the adapter should collect data. Data selection: Select Azure Event Hubs items to which the adapter should subscribe for data. Logging: Set up the logging attributes to manage the adapter logging behavior. For more details, see PI Adapter for Azure Event Hubs data source configuration and PI Adapter for Azure Event Hubs data selection configuration . Azure requirements In addition to the configuration of a data source, data selection, and logging, you need to configure an Event Hub and an Azure Blob Storage account. Azure Blob Storage is required to store the previously read checkpoint for PI Adapter for Azure Event Hubs in case of disconnection. For more information on how to create an event hub, see Quickstart: Create an event hub using Azure portal and for more information on how to create a storage account, see Create a storage account . Connection The adapter communicates with the Azure Event Hubs platform using the AMQP (Advanced Message Queueing Protocol) . Alternatively, the adapter can connect using AMQP over WebSockets using HTTPS protocol with corresponding adapter configuration. A Shared Access Signature (SAS) is required for the adapter to authenticate with the Azure Event Hub namespace, which is supplied by a valid connection string in the adapter\u0027s data source configuration. For more information, see PI Adapter for Azure Event Hubs data source configuration . Data collection After you configure data source and data selection items, the adapter establishes a connection to each event hub within the event hub namespace for all event hubs specified in data selection items. Once a connection is established, the adapter begins consuming events from the event hubs and processes the events as soon as they are published by an event producer and become available to consumers. For more information see PI Adapter for Azure Event Hubs data selection configuration . Data types The following table lists PI Adapter for Azure Event Hubs supported data type names (case-insensitive) and types of streams created. Event Hubs data type names Stream data type Boolean Boolean Int64 Int64 Int32 Int32 Int16 Int16 UInt64 UInt64 UInt32 UInt32 UInt16 UInt16 Float64 Float64 Float32 Float32 Float16 Float32 Date-Time DateTime String String Note: PI Adapter for Azure Event Hubs also supports a limited number of complex data types. For more information, see Complex data type field mapping examples . Stream creation The Azure Event Hubs adapter creates a stream value with two properties for each selected Azure Event Hubs item. The properties are described in the following table. Property name Data type Description Timestamp String The response time of the stream data from the Azure Event Hubs device Value Specified by the data selection The value of the stream data from the Azure Event Hubs device Certain metadata are sent with each created stream. The following metadata are common for every adapter type: ComponentId : Specifies the data source, for example, EventHubs1 ComponentType : Specifies the type of adapter, for example, EventHubs The following metadata are specific to Azure Event Hubs: EventHubName : Contains the Azure Event Hub name configured in the data selection item DeviceId : Contains the Device Id configured in the data selection item (IoT Hub integration only) Note: A configured metadata level allows you to set the amount of metadata for the adapter. Specify the metadata level in General configuration . For the Azure Event Hubs adapter, the following metadata are sent for the individual level: None : No metadata Low : AdapterType (ComponentType) and DataSource (ComponentId) Medium : AdapterType (ComponentType), DataSource (ComponentId), EventHubName, and DeviceId Each stream created for the selected measurement has a unique identifier (stream ID). If you specify a custom stream ID for the measurement in the data selection configuration, the adapter uses that stream ID to create the stream. Otherwise, the adapter constructs the stream ID using the following format: \u003cId\u003e.\u003cValueField\u003e Note: Naming convention is affected by StreamIdPrefix and DefaultStreamIdPattern settings in the data source configuration. For more information, see PI Adapter for Azure Event Hubs data source configuration ."
                                                                                                                      },
    "content/release-notes/release-notes.html":  {
                                                     "href":  "content/release-notes/release-notes.html",
                                                     "title":  "Release notes",
                                                     "keywords":  "Release notes PI Adapter for Azure Event Hubs 1.0.1.239 Adapter Framework 1.4 Overview We are pleased to announce the release of PI Adapter for Azure Event Hubs 1.0.1.239 . This is the first general availability release for the adapter. Resolved issues The following issues have been resolved since the lighthouse release: Item Description 197937 When submitting a POST for data selection configurations to the adapter that contains more than 10,000 streams, a timeout no longer occurs. Setup System requirements Refer to System requirements . Installation Refer to Install the adapter . Uninstallation Refer to Uninstall the adapter . Security information and guidance OSIsoft\u0027s commitment Because the PI System often serves as a barrier protecting control system networks and mission-critical infrastructure assets, OSIsoft is committed to (1) delivering a high-quality product and (2) communicating clearly what security issues have been addressed. This release of PI Adapter for Azure Event Hubs is the highest quality and most secure version of the PI Adapter for Azure Event Hubs released to date. OSIsoft\u0027s commitment to improving the PI System is ongoing, and each future version should raise the quality and security bar even further. Vulnerability communication The practice of publicly disclosing internally discovered vulnerabilities is consistent with the Common Industrial Control System Vulnerability Disclosure Framework developed by the Industrial Control Systems Joint Working Group (ICSJWG) . Despite the increased risk posed by greater transparency, OSIsoft is sharing this information to help you make an informed decision about when to upgrade to ensure your PI System has the best available protection. For more information, refer to OSIsoft\u0027s Ethical Disclosure Policy . To report a security vulnerability, refer to OSIsoft\u0027s Report a Security Vulnerability . Vulnerability scoring OSIsoft has selected the Common Vulnerability Scoring System (CVSS) to quantify the severity of security vulnerabilities for disclosure. To calculate the CVSS scores, OSIsoft uses the National Vulnerability Database (NVD) calculator maintained by the National Institute of Standards and Technology (NIST). OSIsoft uses Critical, High, Medium, and Low categories to aggregate the CVSS Base scores. This removes some of the opinion-related errors of CVSS scoring. As noted in the CVSS specification , Base scores range from 0 for the lowest severity to 10 for the highest severity. Overview of new vulnerabilities found or fixed This section is intended to provide relevant security-related information to guide your installation or upgrade decision. OSIsoft is proactively disclosing aggregate information about the number and severity of PI Adapter for Azure Event Hubs security vulnerabilities that are fixed in this release. No known security vulnerabilities are applicable to this release. The following table lists the known vulnerabilities fixed by this release. Component Version CVE or Reference CVSS Mitigation json.Net 12.0.3 Applications that use Newtonsoft.Json might be exposed to DOS vulnerability 6.7 No code paths result in JSON parsing and subsequent serialization resulting in DoS vulnerability. This vulnerability was resolved by upgrading json.Net to version 13.0.1. Technical support and resources Refer to Technical support and feedback ."
                                                 },
    "content/troubleshooting/troubleshoot-pi-adapter-for-azure-event-hubs.html":  {
                                                                                      "href":  "content/troubleshooting/troubleshoot-pi-adapter-for-azure-event-hubs.html",
                                                                                      "title":  "Troubleshoot PI Adapter for Azure Event Hubs",
                                                                                      "keywords":  "Troubleshoot PI Adapter for Azure Event Hubs PI Adapter for Azure Event Hubs provides troubleshooting features that enable you to verify adapter configuration, confirm connectivity, and view message logs. If you are unable to resolve issues with the adapter or need additional guidance, contact OSIsoft Technical Support through the OSIsoft Customer Portal . Check configurations Incorrect configurations can interrupt data flow and cause errors in values and ranges. Perform the following steps to confirm correct configuration for your adapter. If the adapter is failing to collect any data, navigate to Data source and verify that the following parameter values are correct. Parameter Description StreamIdPrefix If you have recently updated this prefix setting, you must restart the adapter for the change to take effect. EventHubNamespaceConnectionString Verify that the string is entered correctly. If the string is entered incorrectly, the adapter cannot connect to the AEH namespace. ConsumerGroupName Verify that the group name is entered correctly. If the group name is entered incorrectly, the adapter cannot read the event stream. BlobStorageConnectionString Verify that the string is entered correctly. If the string is entered incorrectly, the adapter cannot perform checkpointing, which disrupts data collection. CheckpointBlobContainerName Verify that the container name is entered correctly. If the name is entered incorrectly, the adapter cannot perform checkpointing, which disrupts data collection. If specific data streams or sub-streams are not updating, navigate to Data selection and verify that the following data selection items are correct: Parameter Description StreamId If you using a custom stream ID that is not being generated, verify that it follows the stream ID rules . If the custom stream ID does not follow the rules, the adapter generates a default stream ID based on the measurement configuration. EventHubName The event hub name is valid. If the name is invalid, the adapter cannot collect data. ValueField The JSONPath expression is valid. With an invalid JSONPath expression, the adapter cannot extract a data value from the AEH payload. IndexField The JSONPath expression is valid. With an invalid JSONPath expression, the adapter cannot extract a timestamp from the AEH payload. DataType The correct data type is referenced. An incorrect data type causes data conversion to fail. IndexFormat The correct time format is referenced. A time format that does not match the value from IndexField means that the adapter cannot convert timestamp from the AEH payload. If the adapter is performing suboptimal, navigate to Client settings and review the configuration for any custom values that override default settings. Default client settings fit most use cases, so customized settings are more likely to cause issues. Navigate to Egress endpoints . For each configured endpoint, verify that the Endpoint and authentication properties are correct. For a PI server endpoint, verify UserName and Password . For an OCS endpoint, verify ClientId and ClientSecret . Check connectivity Perform the following steps to verify active connections to the data source and egress endpoints. Based on your egress endpoints, verify that data values are updating. For PI Server, send a request to the PI Web API to verify that PI point values are updating. Use Postman or a Web browser to send the request. For more information, see PI Web API Reference . Alternatively, use any PI Client software to read point values from the PI Data Archive directly. For OCS, view the OCS portal to verify that data streams are updating. For more information, see Getting started with trend data . Alternatively, you can use Postman to send an API request to verify data streams. For more information, see API calls for reading data . If configured, use a health endpoint to determine the status of the adapter. For more information, see Health and diagnostics . Check logs Perform the following steps to view the adapter and endpoint logs to isolate issues for resolution. Navigate to the logs directory: Windows: %ProgramData%\\OSIsoft\\Adapters\\EventHubs\\Logs Linux: /usr/share/OSIsoft/Adapters/EventHubs/Logs  usr share OSIsoft Adapters EventHubs Logs Optional: Change the log level of the adapter to receive more information and context. For more information, see Logging configuration ."
                                                                                  },
    "pi-adapter-for-azure-event-hubs-overview.html":  {
                                                          "href":  "pi-adapter-for-azure-event-hubs-overview.html",
                                                          "title":  "PI Adapter for Azure Event Hubs overview",
                                                          "keywords":  "PI Adapter for Azure Event Hubs overview PI Adapter for Azure Event Hubs is a data-collection component that transfers time-series data from Event Hubs to OMF endpoints in OSIsoft Cloud Services or PI Servers. Event Hubs is a data streaming platform and event ingestion service provided by Microsoft as part of the Azure Cloud platform. The adapter can connect to any Event Hub hosted in Azure that uses the Advanced Message Queueing Protocol (AMQP). Adapter installation You can install the adapter with a download kit that you can obtain from the OSIsoft Customer Portal. You can install the adapter on devices running either Windows or Linux operating systems. Adapter configuration Using the REST API, you can configure all functions of the adapter. The configurations are stored in JSON files. For data ingress, you must define an adapter component in the system components configuration for each device to which the adapter will connect. You configure each adapter component with the connection information for the device and the data to collect. For data egress, you must specify destinations for the data, including security for the outgoing connection. Additional configurations are available to egress health and diagnostics data, add buffering configuration to protect against data loss, and record logging information for troubleshooting purposes. Once you have configured the adapter and it is sending data, you can use administration functions to manage the adapter or individual ingress components of the adapter. Health and diagnostics functions monitor the status of connected devices, adapter system functions, the number of active data streams, the rate of data ingress, the rate of errors, and the rate of data egress. EdgeCmd utility OSIsoft also provides the EdgeCmd utility, a proprietary command line tool to configure and administer an adapter on both Linux and Windows operating systems. EdgeCmd utility is installed separately from the adapter."
                                                      },
    "README.html":  {
                        "href":  "README.html",
                        "title":  "PI Adapter for Azure Event Hubs",
                        "keywords":  "PI Adapter for Azure Event Hubs PI Adapter for Azure Event Hubs is a data-collection component that transfers time-series data from source devices to OMF (OSIsoft Message Format) endpoints in OSIsoft Cloud Services or PI Servers. This repository contains the documentation for PI Adapter for Azure Event Hubs. You can access a readable version of this documentation here. Subtree This documentation repository consumes the PI-Adapter repository as a subtree. This repository contains a documentation framework for adapters. This subtree should be updated periodically. To update the subtree, enter the following command: git subtree pull --prefix content/main content main https://github.com/osisoft/PI-Adapter https:  github.com osisoft PI-Adapter main --squash License ?? 2020 - 2021 OSIsoft, LLC. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 http:  www.apache.org licenses LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
                    }
}
